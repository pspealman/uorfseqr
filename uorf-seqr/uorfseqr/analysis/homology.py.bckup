import matplotlib
# Force matplotlib to not use any Xwindows backend.
# matplotlib.use('Agg')

FDR_RECALL = 0.05
PERMUTATION_NULL = False

# set([all_results.values()[0].tbl[x] for x in numpy.where(all_SPREADS[d].NICE)[0]])&set(golden.keys())

import string
import os
import numpy
import scipy
import scipy.stats
import sys
import pylab
# from intervaltree import Interval, IntervalTree
from collections import namedtuple
import itertools

import simplejson as json

from skippy import *
from common_collapse import *

#import imp
#print(imp.find_module('common_collapse'))
#1/0

import cPickle

if not os.path.exists('results'):
	os.mkdir('results')

gold_standard = []
#for line in open('TESTED-gen1.bed'):
#	print(line)
for line in open('/usr1/home/pspealman/uorf-seqr/uorfs/data/labelled_uorfs/STANDARD-golden.bed'):
	Line = line.rstrip().split()
	gold_standard.append((Line[0], Line[3].split('.')[0], int(Line[1]), int(Line[2]), Line[5]))

golden = { x: ii for (ii,x) in enumerate(gold_standard) }


MCMANUS_SETTINGS = json.load(open('/usr1/home/pspealman/uorf-seqr/uorfs/scer.demo/analysis.json','r'))

feat_order = []
for line in open(MCMANUS_SETTINGS['PREFIX']+'/here.forder'):
	feat_order.append(line.rstrip())

dfeat_order = []
for line in open(MCMANUS_SETTINGS['PREFIX']+'/here.dorder'):
	dfeat_order.append(line.rstrip())

dfeat__protection_sum = dfeat_order.index('protection_sum')

(feat1, dir1, feat2, dir2) = (feat_order.index('abs_phase_of_max_power_freq'), False, feat_order.index('normalized_start_magnitude'), True)
# with open('results/CHOSEN-BY-XVAL.feats') as f:
# 	Line = f.next().rstrip().split()
# (feat1, dir1, feat2, dir2) = (int(Line[0]), Line[2]=='True', int(Line[1]), Line[3]=='True')


# datasets = ['scer.cjm.28/analysis.json']
datasets = ['scer.demo/analysis.json']
#datasets = ['scer.pseudocjm/analysis.json','scer.pseudocjm_2/analysis.json','scer.pseudocjm_3/analysis.json'] 
#datasets = ['scer.blankv2_1/analysis.json','scer.blankv2_2/analysis.json','scer.blankv2_3/analysis.json','scer.blankv2_4/analysis.json','scer.blankv2_5/analysis.json','scer.blankv2_7/analysis.json','scer.blankv2_8/analysis.json']#,'scer.blankv2_6/analysis.json']
#datasets = ['scer.nedialkova_rap/analysis.json']

import itertools
all_results = {}
all_null_results = {}
datasets_order = []
for adataset in datasets:
	x,y = load_data_from_settings(json.load(open(adataset)), feat1, dir1, feat2, dir2, dfeat__protection_sum, force_replay=False)
	for a,b in x:
		all_results[a] = b
		datasets_order.append(a)
	if not PERMUTATION_NULL:
		for a,b in y:
			all_null_results.setdefault(a,[]).append(b)


all_settings = { x : json.load(open(x)) for x in datasets }

standardized_per_sample = {}
for x,y in all_results.iteritems():
	mean_averaged_feats = numpy.mean(y.feats,axis=0)
	std_averaged_feats = numpy.std(y.feats,axis=0)
	standardized_per_sample[x] = (mean_averaged_feats, std_averaged_feats)


def extended_features(b):
	# znew = [z for z in itertools.combinations(range(b.feats.shape[1]),2)]
	# pairfeats = numpy.zeros((b.feats.shape[0], len(znew)))
	# for ii,(i,j) in enumerate(znew):
	# 	pairfeats[:,ii] = numpy.sqrt(numpy.abs(b.feats[:,i]*b.feats[:,j]))
	# return RESULTS(b.tbl, numpy.hstack([b.feats, pairfeats]), b.dfeats, b.countdata, b.freqdata, b.NSAMPLES)
	return b

all_standardized_results = { x : extended_features(apply_zscore(y, standardized_per_sample[x][0], standardized_per_sample[x][1])) for (x,y) in all_results.iteritems() }

def create_permutation_null(y):
	nfeats = numpy.zeros(y.feats.shape)
	idxn = numpy.arange(y.feats.shape[0])
	for j in xrange(y.feats.shape[1]):
		numpy.random.shuffle(idxn)
		nfeats[:,j] = y.feats[idxn,j]
	return RESULTS(y.tbl, nfeats, y.dfeats, y.countdata, y.freqdata, y.NSAMPLES)


all_permutation_null_results = {}
for x,y in all_results.iteritems():
	yy = create_permutation_null(apply_zscore(y, standardized_per_sample[x][0], standardized_per_sample[x][1]))
	all_permutation_null_results[x] = extended_features(yy)


if not PERMUTATION_NULL:
	all_null_results = { x : combine_results(ys) for (x,ys) in all_null_results.iteritems() }
	all_null_results = { x : extended_features(apply_zscore(y, standardized_per_sample[x][0], standardized_per_sample[x][1])) for (x,y) in all_null_results.iteritems() }
else:
	for d in all_standardized_results.keys():
		nfeats = numpy.zeros(all_standardized_results[d].feats.shape)
		for j in xrange(nfeats.shape[1]):
			nfeats[:,j] = all_standardized_results[d].feats[numpy.random.random_integers(0,nfeats.shape[0]-1, size=nfeats.shape[0]),j]
			all_null_results[d] = RESULTS(all_standardized_results[d].tbl, nfeats, all_standardized_results[d].dfeats, all_standardized_results[d].countdata, all_standardized_results[d].freqdata, all_standardized_results[d].NSAMPLES)


all_iso_results = {}
all_noniso_results = {}
for x,y in all_standardized_results.iteritems():
	print 'isolating in',x
	a,b = partition_for_isolates(y)
	all_iso_results[x] = a
	all_noniso_results[x] = b


import rpy2
import rpy2.robjects
import rpy2.robjects as ro
import rpy2.robjects.numpy2ri
rpy2.robjects.numpy2ri.activate()
ro.r("library(glmpath)")

# iso_results = all_iso_results['McManus_S.cerevisiae']
# noniso_results = all_noniso_results['McManus_S.cerevisiae']

# fractions = numpy.linspace(0,1,100)
# gene_buckets = { x : ii for (ii,x) in enumerate(set([x[1] for x in iso_results.tbl])) }
# bucket_of = numpy.array([gene_buckets[x[1]] for x in iso_results.tbl])
# from sklearn.cross_validation import LabelKFold
# NFOLDS = 10
# mycv = LabelKFold(bucket_of, n_folds=NFOLDS)
# errs = numpy.zeros((NFOLDS, len(fractions)))
# hoerrs = numpy.zeros((NFOLDS, len(fractions)))
# for vv,(trainidxn, testidxn) in enumerate(mycv):
# 	fit = ro.r.glmpath(iso_results.feats[trainidxn,:],iso_results.freqdata[trainidxn],family="binomial")
# 	# predictions = numpy.asarray(ro.r("predict.glmpath")(fit, iso_results.feats[testidxn], s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False, frac_arclength=0.5))
# 	predictions = numpy.asarray(ro.r("predict.glmpath")(fit, iso_results.feats[testidxn], s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False))
# 	errs[vv,:] = numpy.sum((predictions-iso_results.freqdata[testidxn,numpy.newaxis])**2 ,axis=0)
# 	predictions = numpy.asarray(ro.r("predict.glmpath")(fit, noniso_results.feats, s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False))
# 	hoerrs[vv,:] = numpy.sum((predictions-noniso_results.freqdata[:,numpy.newaxis])**2 ,axis=0)

# fit2 = ro.r.glmpath(iso_results.feats,iso_results.freqdata,family="binomial")
# mytau = numpy.argmin(numpy.mean(errs,axis=0))


# predictions = numpy.asarray(ro.r("predict.glmpath")(fit2, iso_results.feats, s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False))
# numpy.sum((predictions[:,99]-iso_results.freqdata)**2)

# predictions2 = numpy.asarray(ro.r("predict.glmpath")(fit2, all_iso_results['McManus_S.paradoxus'].feats, s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False))
# numpy.sum((predictions2[:,99]-all_iso_results['McManus_S.paradoxus'].freqdata)**2)



import statsmodels.api as sm
# this only makes sense when you have more than one sample
per_sample_regressions = {}
REGRESSION = namedtuple('REGRESSION', 'fit2 mytau')
globals()[REGRESSION.__name__] = REGRESSION

def train_regression(iso_results):
	# themodel = sm.GLM(iso_results.freqdata, iso_results.feats, family=sm.families.Binomial())
	# themodel_results = themodel.fit()
	# the method used by glmnet and glmpath is to use the lambdas at a fixed percentage of the maximum lambda
	# which is a super clever way to do this
	fractions = numpy.linspace(0,1,100)
	gene_buckets = { x : ii for (ii,x) in enumerate(set([x[1] for x in iso_results.tbl])) }
	bucket_of = numpy.array([gene_buckets[x[1]] for x in iso_results.tbl])
	from sklearn.cross_validation import LabelKFold
	NFOLDS = 10
	mycv = LabelKFold(bucket_of, n_folds=NFOLDS)
	errs = numpy.zeros((NFOLDS, len(fractions)))
	for vv,(trainidxn, testidxn) in enumerate(mycv):
		fit = ro.r.glmpath(iso_results.feats[trainidxn,:],iso_results.freqdata[trainidxn],family="binomial")
		predictions = numpy.asarray(ro.r("predict.glmpath")(fit, iso_results.feats[testidxn], s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False))
		errs[vv,:] = numpy.sum((predictions-iso_results.freqdata[testidxn,numpy.newaxis])**2 ,axis=0)
	fit2 = ro.r.glmpath(iso_results.feats,iso_results.freqdata,family="binomial")
	mytau = numpy.argmin(numpy.mean(errs,axis=0))
	# return themodel_results
	return REGRESSION(fit2, mytau)


if not os.path.exists('regression'):
	os.mkdir('regression')

for sname, asampl in all_results.iteritems():
	if asampl.NSAMPLES > 1:
		if not os.path.exists('regression/'+sname+'.glmregression'):
			print 'working on', sname
			fit2 = train_regression(all_iso_results[sname])
			per_sample_regressions[sname] = fit2
			cPickle.dump(per_sample_regressions[sname], open('regression/'+sname+'.glmregression','w'))
		else:
			print '...recovering regression from', sname
			per_sample_regressions[sname] = cPickle.load(open('regression/'+sname+'.glmregression','r'))


# rumi 20170529 dump the regression coefficients clearly
fout = open('results/feature_weights_per_experiment.csv','w')
fout.write(', '.join(['experiment name', 'Intercept']+feat_order)+"\n")
for x,y in per_sample_regressions.iteritems():
	atau = y.mytau
	areg = y.fit2
	vv = numpy.array(ro.r("predict.glmpath")(areg, s=ro.r.seq(0,1,length=100), type="coefficients", mode='norm.fraction', standardize=False))[atau,:]
	fout.write(x+', '+', '.join([str(a) for a in vv])+"\n")

fout.close()


multiple_sample_datasets = sorted(per_sample_regressions.keys())

# for x,y in per_sample_regressions.iteritems():
# 	print x
# 	print numpy.asarray(ro.r("predict.glmpath")(y.fit2, s=ro.r.seq(0,1,length=100), type="coefficients", mode='norm.fraction', standardize=False))[y.mytau,:]


# BUILD THE SPREADS

datasets_ordered_by_size = sorted(datasets_order, key=lambda x: all_results[x].NSAMPLES, reverse=True)

nontrivial_datasets_ordered_by_size = [x for x in datasets_ordered_by_size if all_results[x].NSAMPLES>1]


from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
from sklearn.cross_validation import StratifiedKFold

SPREAD = namedtuple('SPREAD', 'nonnull score_real score_null score_perm probs_real probs_null probs_perm score_cutoff probs_cutoff NICE qvals')
globals()[SPREAD.__name__] = SPREAD

def bisect_right(a, x, lo=0, hi=None):
	if lo < 0:
		raise ValueError('lo must be non-negative')
	if hi is None:
		hi = len(a)
	while lo < hi:
		mid = (lo+hi)//2
		if numpy.any(x < a[mid]):
			lo = mid+1
		else: 
			hi = mid
		# print x, a[mid], lo, hi, mid
	return lo

def assess_result(regress, obs_RESULT, null_RESULT, perm_RESULT):
	y = numpy.hstack([numpy.ones((obs_RESULT.feats.shape[0],)), numpy.zeros((null_RESULT.feats.shape[0],))])
	gene_buckets = { x : ii for (ii,x) in enumerate(set([]).union(*[set([x[1] for x in z]) for z in [obs_RESULT.tbl, null_RESULT.tbl]])) }
	bucket_of = numpy.array([gene_buckets[x[1]] for x in obs_RESULT.tbl]+[gene_buckets[x[1]] for x in null_RESULT.tbl])
	working = numpy.vstack([obs_RESULT.feats, null_RESULT.feats])
	# from sklearn.cross_validation import LabelKFold
	# NFOLDS = 10
	# mycv = LabelKFold(bucket_of, n_folds=NFOLDS)
	# # clf = LogisticRegressionCV(cv=mycv, penalty='l2', solver='newton-cg', n_jobs=-1, refit=False, tol=0.01, verbose=5, class_weight={0 : 0.5, 1:0.5})
	# import pdb; pdb.set_trace()
	# clf = LogisticRegression(penalty='l2', solver='sag', n_jobs=-1, tol=0.01, verbose=5, class_weight={0 : 0.5, 1:0.5})
	from sklearn.ensemble import RandomForestClassifier
	clf = RandomForestClassifier(n_estimators=101, class_weight={0 : 0.5, 1:0.5}, oob_score=False, n_jobs=-1)
	clf.fit(numpy.vstack([obs_RESULT.feats, null_RESULT.feats]), y)
	# vv = clf.predict(numpy.vstack([obs_RESULT.feats, null_RESULT.feats]))
	# clf.score(numpy.vstack([obs_RESULT.feats, null_RESULT.feats]), y)
	#
	probs_real = clf.predict_proba(obs_RESULT.feats)[:,1]
	probs_null = clf.predict_proba(null_RESULT.feats)[:,1]
	probs_perm = clf.predict_proba(perm_RESULT.feats)[:,1]
	# score_real = regress.fit2.predict(obs_RESULT.feats)
	# score_null = regress.fit2.predict(null_RESULT.feats)
	# score_perm = regress.fit2.predict(perm_RESULT.feats)

	score_real = numpy.asarray(ro.r("predict.glmpath")(regress.fit2, obs_RESULT.feats, s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False))[:,regress.mytau]
	score_null = numpy.asarray(ro.r("predict.glmpath")(regress.fit2, null_RESULT.feats, s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False))[:,regress.mytau]
	score_perm = numpy.asarray(ro.r("predict.glmpath")(regress.fit2, perm_RESULT.feats, s=ro.r.seq(0,1,length=100), type="response", mode='norm.fraction', standardize=False))[:,regress.mytau]

	stacked = numpy.vstack([score_perm, probs_perm]).T
	thresholds = numpy.sort(stacked,axis=0)[::-1]
	hh = numpy.zeros((thresholds.shape[0]+1,))
	for x in stacked:
		hh[bisect_right(thresholds, x)] +=1
	hh = numpy.cumsum(hh)/stacked.shape[0]
	real_bundle = numpy.vstack([score_real, probs_real]).T
	pvals = numpy.ones((score_real.shape[0],))
	for ii in xrange(real_bundle.shape[0]):
		pvals[ii] = hh[bisect_right(thresholds, real_bundle[ii])]
	spvals = numpy.argsort(pvals)
	#G'Sell et al. ForwardStop
	unsorted_qvals = -numpy.cumsum(numpy.log(1-pvals[spvals]))/numpy.arange(1, pvals.shape[0]+1)
	final_qval = numpy.max(numpy.where(unsorted_qvals<=FDR_RECALL)[0])
	score_cutoff, probs_cutoff = thresholds[final_qval]
	qvals = numpy.zeros((pvals.shape[0],))
	qvals[spvals] = numpy.minimum(unsorted_qvals, 1.0)
	# resolve across replicates by choosing the very best score in overlapping predictions
	preNICE = numpy.where(qvals<=FDR_RECALL)[0]
	per_gene = {}
	for z,zz in [(obs_RESULT.tbl[x],x) for x in preNICE]:
		per_gene.setdefault(z[1],[]).append(zz)
	NICE = numpy.zeros((score_real.shape[0],),dtype='bool')
	for g,xs in per_gene.iteritems():
		if len(xs)>1:
			xs = sorted(xs, key=lambda x:score_real[x], reverse=True)
			t = skippy.IntervalSkiplist(3)
			for x in xs:
				y = obs_RESULT.tbl[x]
				if not t.is_interval_contained(y[2], y[3]):
					t.insert_interval(x, y[2], y[3])
			NICE[t.lookup.keys()] = True
		else:
			NICE[xs] = True
	return SPREAD(clf, score_real, score_null, score_perm, probs_real, probs_null, probs_perm, score_cutoff, probs_cutoff, NICE, qvals)


all_SPREADS = {}
for ii,d in enumerate(nontrivial_datasets_ordered_by_size):
	if not os.path.exists('regression/'+d+'.glmspread'):
		print 'working on spread for', d
		all_SPREADS[d] = assess_result(per_sample_regressions[d], all_standardized_results[d], all_null_results[d], all_permutation_null_results[d])
		cPickle.dump(all_SPREADS[d], open('regression/'+d+'.glmspread','w'))
	else:
		print '...recovering spread for', d
		all_SPREADS[d] = cPickle.load(open('regression/'+d+'.glmspread','r'))


##### Produce invidiual dataset scatter plots

# obscolors = ['#fee8c8', '#fdbb84', '#e34a33']
# nullcolors = ['#e0ecf4', '#9ebcda', '#8856a7']
obscolors = ['#e34a33', '#e34a33', '#e34a33']
nullcolors = ['#8856a7', '#8856a7', '#8856a7']


def plotagainst(dname, ax1, ax2, obs_RESULT, null_RESULT, the_SPREAD, i):
	if numpy.max(null_RESULT.countdata)<i:
		return
	ax1.set_title(dname)
	ax1.plot([the_SPREAD.probs_cutoff, the_SPREAD.probs_cutoff], [0,1], '-', linewidth=1, color='0.80')
	ax1.plot([0,1], [the_SPREAD.score_cutoff, the_SPREAD.score_cutoff], '-', linewidth=1, color='0.80')
	# for i in range(1,4):
	# ax1.plot(the_SPREAD.probs_null[null_RESULT.countdata==i], the_SPREAD.score_null[null_RESULT.countdata==i], '.', markersize=2, color=nullcolors[i-1])
	ax1.hexbin(the_SPREAD.probs_null[null_RESULT.countdata==i], the_SPREAD.score_null[null_RESULT.countdata==i], mincnt=1, marginals=False, cmap='Blues', bins='log', gridsize=50)
	#
	ax1.set_xticks([0.25, 0.5, 0.75, 1.0])
	ax1.set_yticks([0.25, 0.5, 0.75, 1.0])
	ax1.set_xlim([0,1])
	ax1.set_ylim([0,1])
	ax1.set_xlabel('non-null projection', fontsize=9)
	ax1.set_ylabel('regression score', fontsize=9)
	ax2.plot([the_SPREAD.probs_cutoff, the_SPREAD.probs_cutoff], [0,1], '-', linewidth=1, color='0.80')
	ax2.plot([0,1], [the_SPREAD.score_cutoff, the_SPREAD.score_cutoff], '-', linewidth=1, color='0.80')
	# for i in range(1,4):
	# ax2.plot(the_SPREAD.probs_real[obs_RESULT.countdata==i], the_SPREAD.score_real[obs_RESULT.countdata==i], '.', markersize=2, color=obscolors[i-1])
	ax2.hexbin(the_SPREAD.probs_real[obs_RESULT.countdata==i], the_SPREAD.score_real[obs_RESULT.countdata==i], mincnt=1, marginals=False, cmap='Reds', bins='log', gridsize=50)
	Q = [(obs_RESULT.countdata[ii], the_SPREAD.probs_real[ii], the_SPREAD.score_real[ii]) for (ii,x) in enumerate(obs_RESULT.tbl) if x in gold_standard]
	if len(Q)>0:
		Q = numpy.vstack(Q)
		ax2.plot(Q[:,1], Q[:,2], 'k.')
	ax2.set_xticks([0.25, 0.5, 0.75, 1.0])
	ax2.set_yticks([0.25, 0.5, 0.75, 1.0])
	ax2.set_xlim([0,1])
	ax2.set_ylim([0,1])
	ax2.set_xlabel('non-null projection', fontsize=9)
	ax2.set_ylabel('regression score', fontsize=9)

#PS - commenting out after error, potentially due to recent server wipe and reload 05.18.17
#for j in range(1,4):
#	fig = pylab.figure(figsize=(8,8*len(nontrivial_datasets_ordered_by_size)/2.0))
#	for ii,d in enumerate(nontrivial_datasets_ordered_by_size):
#		plotagainst(d, fig.add_subplot(len(nontrivial_datasets_ordered_by_size),2,ii*2+1), fig.add_subplot(len(nontrivial_datasets_ordered_by_size),2,ii*2+2), all_results[d], all_null_results[d], all_SPREADS[d],j)
#	pylab.tight_layout()
#	pylab.savefig('results/sensu-stricto-score-performances-'+str(j)+'.png', bbox='tight')
#	pylab.close()



def plotagainst(dname, ax1, ax2, obs_RESULT, null_RESULT, the_SPREAD, i):
	if numpy.max(null_RESULT.countdata)<i:
		return
	ax1.set_title(dname)
	ax1.plot([the_SPREAD.probs_cutoff, the_SPREAD.probs_cutoff], [0,1], '-', linewidth=1, color='0.80')
	ax1.plot([0,1], [the_SPREAD.score_cutoff, the_SPREAD.score_cutoff], '-', linewidth=1, color='0.80')
	# for i in range(1,4):
	# ax1.plot(the_SPREAD.probs_perm[null_RESULT.countdata==i], the_SPREAD.score_perm[null_RESULT.countdata==i], '.', markersize=2, color=nullcolors[i-1])
	ax1.hexbin(the_SPREAD.probs_perm[null_RESULT.countdata==i], the_SPREAD.score_perm[null_RESULT.countdata==i], mincnt=1, marginals=False, cmap='Blues', bins='log', gridsize=50)
	#
	ax1.set_xticks([0.25, 0.5, 0.75, 1.0])
	ax1.set_yticks([0.25, 0.5, 0.75, 1.0])
	ax1.set_xlim([0,1])
	ax1.set_ylim([0,1])
	ax1.set_xlabel('non-null projection', fontsize=9)
	ax1.set_ylabel('regression score', fontsize=9)
	ax2.plot([the_SPREAD.probs_cutoff, the_SPREAD.probs_cutoff], [0,1], '-', linewidth=1, color='0.80')
	ax2.plot([0,1], [the_SPREAD.score_cutoff, the_SPREAD.score_cutoff], '-', linewidth=1, color='0.80')
	# for i in range(1,4):
	# ax2.plot(the_SPREAD.probs_real[obs_RESULT.countdata==i], the_SPREAD.score_real[obs_RESULT.countdata==i], '.', markersize=2, color=obscolors[i-1])
	ax2.hexbin(the_SPREAD.probs_real[obs_RESULT.countdata==i], the_SPREAD.score_real[obs_RESULT.countdata==i], mincnt=1, marginals=False, cmap='Reds', bins='log', gridsize=50)
	Q = [(obs_RESULT.countdata[ii], the_SPREAD.probs_real[ii], the_SPREAD.score_real[ii]) for (ii,x) in enumerate(obs_RESULT.tbl) if x in gold_standard]
	if len(Q)>0:
		Q = numpy.vstack(Q)
		ax2.plot(Q[:,1], Q[:,2], 'k.')
	ax2.set_xticks([0.25, 0.5, 0.75, 1.0])
	ax2.set_yticks([0.25, 0.5, 0.75, 1.0])
	ax2.set_xlim([0,1])
	ax2.set_ylim([0,1])
	ax2.set_xlabel('specificity of P site score', fontsize=9)
	ax2.set_ylabel('regression score', fontsize=9)

#PS - commmented out after error - possibly due to server wipe and reload 05.18.17
#for j in range(1,4):
#	fig = pylab.figure(figsize=(8,8*len(nontrivial_datasets_ordered_by_size)/2.0))
#	for ii,d in enumerate(nontrivial_datasets_ordered_by_size):
#		plotagainst(d, fig.add_subplot(len(nontrivial_datasets_ordered_by_size),2,ii*2+1), fig.add_subplot(len(nontrivial_datasets_ordered_by_size),2,ii*2+2), all_results[d], all_permutation_null_results[d], all_SPREADS[d],j)
#	pylab.tight_layout()
#	pylab.savefig('results/perm-sensu-stricto-score-performances-'+str(j)+'.png', bbox='tight')#
#	pylab.close()


### dump sufficient statistics


for d in nontrivial_datasets_ordered_by_size:
	fout_all = open('results/'+d+'-all-qvals-with-pwm-hack.bed','w')
	for ii in range(len(all_results[d].tbl)):
		x = all_results[d].tbl[ii]
		if x[4]=='+':
			jj = str(x[2])
		else:
			jj = str(x[3])
		pre = "\t".join([x[0], str(x[2]), str(x[3]), x[1]+'.'+jj])
		bland = pre+'\t'+str(int(1000*all_SPREADS[d].qvals[ii]))+'\t'+str(x[4])+'\t'
		bland += str(all_results[d].dfeats[ii, dfeat_order.index('pwm_score')])+"\n"
		fout_all.write(bland)
	fout_all.close()

for d in nontrivial_datasets_ordered_by_size:
	fout_all = open('results/'+d+'-all-qvals.bed','w')
	fout_significants = open('results/'+d+'-significant-qvals.bed','w')
	fout_resolved = open('results/'+d+'-significant-resolved-qvals.bed','w')
	for ii in range(len(all_results[d].tbl)):
		x = all_results[d].tbl[ii]
		if x[4]=='+':
			jj = str(x[2])
		else:
			jj = str(x[3])
		pre = "\t".join([x[0], str(x[2]), str(x[3]), x[1]+'.'+jj])
		bland = pre+'\t'+str(int(1000*all_SPREADS[d].qvals[ii]))+'\t'+str(x[4])+"\n"
		fout_all.write(bland)
		if all_SPREADS[d].qvals[ii]<=FDR_RECALL:
			fout_significants.write(bland)
			if all_SPREADS[d].NICE[ii]:
				fout_resolved.write(bland)
	fout_all.close()
	fout_significants.close()
	fout_resolved.close()

for d in nontrivial_datasets_ordered_by_size:
	fout_all = open('results/'+d+'-all-scores.bed','w')
	fout_significants = open('results/'+d+'-significant-scores.bed','w')
	fout_resolved = open('results/'+d+'-significant-resolved-scores.bed','w')
	for ii in range(len(all_results[d].tbl)):
		x = all_results[d].tbl[ii]
		if x[4]=='+':
			jj = str(x[2])
		else:
			jj = str(x[3])
		pre = "\t".join([x[0], str(x[2]), str(x[3]), x[1]+'.'+jj])
		bland = pre+'\t'+str(int(1000*all_SPREADS[d].score_real[ii]))+'\t'+str(x[4])+"\n"
		fout_all.write(bland)
		if all_SPREADS[d].qvals[ii]<=FDR_RECALL:
			fout_significants.write(bland)
			if all_SPREADS[d].NICE[ii]:
				fout_resolved.write(bland)
	fout_all.close()
	fout_significants.close()
	fout_resolved.close()

for d in nontrivial_datasets_ordered_by_size:
	fout_all = open('results/'+d+'-all-specificity.bed','w')
	fout_significants = open('results/'+d+'-significant-specificity.bed','w')
	fout_resolved = open('results/'+d+'-significant-resolved-specificity.bed','w')
	for ii in range(len(all_results[d].tbl)):
		x = all_results[d].tbl[ii]
		if x[4]=='+':
			jj = str(x[2])
		else:
			jj = str(x[3])
		pre = "\t".join([x[0], str(x[2]), str(x[3]), x[1]+'.'+jj])
		bland = pre+'\t'+str(int(1000*all_SPREADS[d].probs_real[ii]))+'\t'+str(x[4])+"\n"
		fout_all.write(bland)
		if all_SPREADS[d].qvals[ii]<=FDR_RECALL:
			fout_significants.write(bland)
			if all_SPREADS[d].NICE[ii]:
				fout_resolved.write(bland)
	fout_all.close()
	fout_significants.close()
	fout_resolved.close()

all_NICE = {}
for d in nontrivial_datasets_ordered_by_size:
	nice_idxn = numpy.where(all_SPREADS[d].NICE)[0]
	newtbl = [all_results[d].tbl[x] for x in nice_idxn]
	newfeats = all_results[d].feats[nice_idxn]
	newdfeats = all_results[d].dfeats[nice_idxn]
	newcountdata = all_results[d].countdata[nice_idxn]
	newfreqdata = all_results[d].freqdata[nice_idxn]
	sigresult = RESULTS(newtbl, newfeats, newdfeats, newcountdata, newfreqdata, all_results[d].NSAMPLES)
	cPickle.dump(sigresult, open('results/NICE_OF-'+d+'.RESULT','w'))
	all_NICE[d] = sigresult


############# cluster datasets by their regression weights


regression_weights = numpy.vstack([numpy.asarray(ro.r("predict.glmpath")(per_sample_regressions[x].fit2, s=ro.r.seq(0,1,length=100), type="coefficients", mode='norm.fraction', standardize=False))[per_sample_regressions[x].mytau,:] for x in nontrivial_datasets_ordered_by_size])

# regression_weights = numpy.vstack([per_sample_regressions[x].fit2.params for x in nontrivial_datasets_ordered_by_size])

import itertools
import heapq

def minimax_cluster(X):
	level = [[x] for x in range(X.shape[0])]
	bests = { x : x for x in range(X.shape[0]) }
	alive = range(X.shape[0])
	alives = []
	pairdists = []
	decisions = []
	from scipy.spatial.distance import cdist
	for a,b in itertools.combinations(alive,2):
		together = level[a]+level[b]
		D = numpy.max(cdist(X[together,:],X[together]),axis=1)
		mm = numpy.argmin(D)
		heapq.heappush(pairdists,(D[mm], (together[mm],a,b)))
	#
	toppoint = X.shape[0]
	Z = []
	while len(alive)>1:
		v, atop = heapq.heappop(pairdists)
		while ( atop[1] not in alive ) or ( atop[2] not in alive ):
			v, atop = heapq.heappop(pairdists)
		print len(alive), len(level)
		alive = [x for x in alive if x not in [atop[1], atop[2]]]
		newish = level[atop[1]]+level[atop[2]]
		level.append(newish)
		for a in alive:
			together = level[a]+newish
			D = numpy.max(cdist(X[together,:],X[together]),axis=1)
			mm = numpy.argmin(D)
			heapq.heappush(pairdists,(D[mm], (together[mm],a, toppoint)))
		alive.append(toppoint)
		alives.append(alive)
		decisions.append((atop[1], atop[2]))
		bests[toppoint] = atop[0]
		Z.append([atop[1], atop[2], v, len(newish)])
		toppoint += 1
	#
	Z = numpy.asarray(Z)
	return Z, alives, level


Z, alives, level = minimax_cluster(regression_weights)

from scipy.cluster.hierarchy import dendrogram, linkage
# Z = linkage(regression_weights, 'average')
dendrogram(Z, labels=nontrivial_datasets_ordered_by_size, orientation='left')
pylab.tight_layout()
pylab.savefig('results/sensu-stricto-regression-weight-clusters.png')
pylab.close()


############# CLUSTER THE UORFS

print 1/0
ro.r.library('tree')
treedatasets = ['scer.cjm', 'spar.cjm', 'sbay.tqt']
# [x for x in enumerate(feat_order)]
treefeats = [0,1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16, 17]
treefeatorder = [feat_order[x] for x in treefeats]

def rankmatrix(xs):
	raw = numpy.hstack([scipy.stats.rankdata(xs[:,j])[:,numpy.newaxis] for j in range(xs.shape[1])])
	raw -= numpy.min(raw,axis=0)
	raw /= numpy.max(raw,axis=0)
	return raw

mfeats = rankmatrix(all_NICE['scer.cjm'].feats[:,treefeats])

# tst = qq(all_NICE['spar.cjm'].feats[:,0])


bin_mfeats = numpy.zeros(mfeats.shape, dtype='int')

# l1 = numpy.percentile(mfeats,25,axis=0)
# l2 = numpy.percentile(mfeats,75,axis=0)
# bin_mfeats[mfeats<l1] = -1
# bin_mfeats[mfeats>l2] = 1

bin_mfeats[mfeats>=numpy.median(mfeats,axis=0)] = 1
bin_mfeats[:,[5,6,7]] = 0
bin_mfeats[mfeats[:,5]!=0,5] = 1
bin_mfeats[mfeats[:,6]!=0,6] = 1
bin_mfeats[mfeats[:,7]!=0,7] = 1


wbin = numpy.zeros((mfeats.shape[0],),dtype='int')
tbins = {}
rev_tbins = []
for ii in range(bin_mfeats.shape[0]):
	tx = tuple(bin_mfeats[ii])
	if tx not in tbins:
		tbins[tx] = len(tbins)
		rev_tbins.append(tx)
	wbin[ii] = tbins[tx]

# numpy.std(numpy.vstack([rev_tbins[x] for x in numpy.where(numpy.bincount(wbin)>6)[0]]),axis=0)

ok_bins = numpy.argsort(numpy.bincount(wbin))[::-1][:31]
ok_to_cluster = numpy.zeros((mfeats.shape[0],),dtype=bool)
for k in ok_bins:
	ok_to_cluster |= wbin==k



dd = {}
for ii,g in enumerate(treefeatorder):
	dd[g] = ro.r.matrix(mfeats[ok_to_cluster,ii])

dd['class'] = ro.r.factor(wbin[ok_to_cluster])


df = ro.DataFrame(dd)
atree = ro.r.tree('class ~ '+'+'.join(treefeatorder), df)


print atree.rx2('frame')
ro.r('misclass.tree')(atree)


full_dd = {}
for ii,g in enumerate(treefeatorder):
	full_dd[g] = ro.r.matrix(mfeats[:,ii])

full_df = ro.DataFrame(full_dd)

newassignments = numpy.array(ro.r('predict')(atree, newdata=full_df, type='where'))


ro.r.pdf('rescale.pdf')
ro.r.plot(atree)
ro.r.text(atree, splits=True, digits=2, cex=0.5)
ro.r('dev.off')()

levels = [x for x in atree.rx2('frame').rx2('var').levels]
rawlevels = [levels[x-1] for x in atree.rx2('frame').rx2('var')]

nodeorder = numpy.array([int(x) for x in ro.r.rownames(atree.rx2('frame'))])
left_to_right_leaves = [x[1] for x in zip(rawlevels, nodeorder) if 'leaf' in x[0]]

actual_leaf_sizes = numpy.bincount(nodeorder[newassignments-1])
left_to_right_leaf_sizes = actual_leaf_sizes[left_to_right_leaves]


nodes_of_ok_to_cluster = nodeorder[newassignments[ok_to_cluster]-1]
left_to_right_minimax = []
left_to_right_medoid = []
ok_to_cluster_idxn = numpy.where(ok_to_cluster)[0]
for x in left_to_right_leaves:
	si = numpy.where(nodes_of_ok_to_cluster==x)[0]
	ss = mfeats[si]
	D = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(ss))
	left_to_right_minimax.append(all_NICE['scer.cjm'].tbl[ok_to_cluster_idxn[si[numpy.argmin(numpy.max(D,axis=1))]]])
	left_to_right_medoid.append(all_NICE['scer.cjm'].tbl[ok_to_cluster_idxn[si[numpy.argmin(numpy.mean(D,axis=1))]]])

fout = open('results/scer_significant_breakdown__DATA.txt','w')
fout.write('# from left-to-right, leaf size, then the minimax center, then the medoid center\n')
for x,y,z in zip(left_to_right_leaf_sizes, left_to_right_minimax, left_to_right_medoid):
	fout.write('\t'.join([str(x), ':'.join([str(q) for q in y]), ':'.join([str(q) for q in z])])+"\n")

fout.close()


fout = open('results/scer_significant_breakdown__DATA_leafassignments.txt','w')
fout.write('# leaf index (read from left to right, so the first one is 1, the second from the left is 2), uorf\n')
for ii, x in enumerate(left_to_right_leaves):
	for j in [all_NICE['scer.cjm'].tbl[q] for q in numpy.where(nodeorder[newassignments-1]==x)[0]]:
		fout.write('\t'.join([str(ii+1), ':'.join([str(q) for q in j])])+"\n")

fout.close()




full_dd_spar = {}
remfeats = rankmatrix(all_NICE['spar.cjm'].feats)
for ii,g in enumerate(treefeatorder):
	full_dd_spar[g] = ro.r.matrix(remfeats[:,ii])

full_df_spar = ro.DataFrame(full_dd_spar)

newassignments = nodeorder[numpy.array(ro.r('predict')(atree, newdata=full_df_spar, type='where'))-1]


U,s,V = numpy.linalg.svd(mfeats-numpy.mean(mfeats,axis=0),full_matrices=False)

ro.r.library('protoclust')
dd = ro.r.dist(mfeats)
hc = ro.r.protoclust(dd)
cut = ro.r.protocut(hc,k=8)
cut.rx2('cl')

dd = {}
for ii,g in enumerate(treefeatorder):
	dd[g] = ro.r.matrix(mfeats[:,ii])

dd['class'] = ro.r.factor(cut.rx2('cl'))


df = ro.DataFrame(dd)
atree = ro.r.tree('class ~ '+'+'.join(treefeatorder), df)


print atree.rx2('frame')
ro.r('misclass.tree')(atree)



ro.r.pdf('classes.pdf')
ro.r.plot(atree,type='uniform')
ro.r.text(atree, splits=True, digits=2, cex=0.5)
ro.r('dev.off')()


newassignments = numpy.array(ro.r('predict')(atree, newdata=df, type='where'))

levels = [x for x in atree.rx2('frame').rx2('var').levels]
rawlevels = [levels[x-1] for x in atree.rx2('frame').rx2('var')]

nodeorder = numpy.array([int(x) for x in ro.r.rownames(atree.rx2('frame'))])
left_to_right_leaves = [x[1] for x in zip(rawlevels, nodeorder) if 'leaf' in x[0]]

actual_leaf_sizes = numpy.bincount(nodeorder[newassignments-1])
left_to_right_leaf_sizes = actual_leaf_sizes[left_to_right_leaves]













# nodes_of_ok_to_cluster = nodeorder[newassignments[ok_to_cluster]-1]
# left_to_right_minimax = []
# left_to_right_medoid = []
# ok_to_cluster_idxn = numpy.where(ok_to_cluster)[0]
# for x in left_to_right_leaves:
# 	si = numpy.where(nodes_of_ok_to_cluster==x)[0]
# 	ss = mfeats[si]
# 	D = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(ss))
# 	left_to_right_minimax.append(all_NICE['scer.cjm'].tbl[ok_to_cluster_idxn[si[numpy.argmin(numpy.max(D,axis=1))]]])
# 	left_to_right_medoid.append(all_NICE['scer.cjm'].tbl[ok_to_cluster_idxn[si[numpy.argmin(numpy.mean(D,axis=1))]]])

# fout = open('results/scer_significant_breakdown__DATA.txt','w')
# fout.write('# from left-to-right, leaf size, then the minimax center, then the medoid center\n')
# for x,y,z in zip(left_to_right_leaf_sizes, left_to_right_minimax, left_to_right_medoid):
# 	fout.write('\t'.join([str(x), ':'.join([str(q) for q in y]), ':'.join([str(q) for q in z])])+"\n")

# fout.close()


# fout = open('results/scer_significant_breakdown__DATA_leafassignments.txt','w')
# fout.write('# leaf index (read from left to right, so the first one is 1, the second from the left is 2), uorf\n')
# for ii, x in enumerate(left_to_right_leaves):
# 	for j in [all_NICE['scer.cjm'].tbl[q] for q in numpy.where(nodeorder[newassignments-1]==x)[0]]:
# 		fout.write('\t'.join([str(ii+1), ':'.join([str(q) for q in j])])+"\n")

# fout.close()


print 1/0














canonized = {}
for ii,d in enumerate(nontrivial_datasets_ordered_by_size):
	canonized[d] = set([all_results[d].tbl[x] for x in numpy.where(all_SPREADS[d].NICE)[0]])

# canonized_giant_counts = numpy.zeros((len(giant_table), len(nontrivial_datasets_ordered_by_size)))
# for jj,d in enumerate(nontrivial_datasets_ordered_by_size):
# 	for x in canonized[d]:
# 		canonized_giant_counts[giant_table[x],jj] = 1

# q = numpy.bincount(numpy.sum(canonized_giant_counts,axis=1).astype('int'))
# for i in range(1,q.shape[0]):
# 	print i, q[i]

# # save the nice ones in a spreadsheet

# import xlwt
# workbook = xlwt.Workbook() 
# sheet = workbook.add_sheet("Significant uORFs")
# # write the column headers
# for ii,x in enumerate(['#chr', 'start', 'end', 'ORF symbol', 'occurrence', 'polarity']+nontrivial_datasets_ordered_by_size):
# 	sheet.write(0,ii,x)

# for ii,jj in enumerate(numpy.argsort(numpy.sum(canonized_giant_counts,axis=1))[::-1]):
# 	me = rev_giant_table[jj]
# 	summ = numpy.sum(canonized_giant_counts[jj,:])
# 	if summ==0:
# 		break
# 	for z,v in enumerate([me[0], str(me[2]), str(me[3]), me[1], int(summ), me[-1]]+[int(v) for v in canonized_giant_counts[jj,:]]):
# 		sheet.write(ii+1,z, v)

# workbook.save('results/sensu-stricto-significant-uorfs-breakdown.xls')


## now for the homology analysis

# ok, we need the genome maps now - doing this inefficently & incorrectly for general use but whatever
import common_formats
per_chromosome_TLS_data = {}
per_chromosome_features_masked_out = {}
unwanted_genes = {}
sequences = {}
genome_of_dataset = {}

astrandmap = { 'negative_strand' : '-', 'positive_strand' : '+' }
for asetting in all_settings.values():
	genome = asetting['GENOME']
	for agroup in [x['groupname'] for x in asetting['SAMPLE_GROUPS']]:
		per_chromosome_TLS_data[agroup] = {}
		per_chromosome_features_masked_out[agroup] = {}
		unwanted_genes[agroup] = set([])
		common_formats.import_gff_file(asetting['GFF_FILE'], per_chromosome_TLS_data[agroup], per_chromosome_features_masked_out[agroup], unwanted_genes[agroup])
		genome_of_dataset[agroup] = genome
	sequences[genome] = {}
	for x in os.listdir(asetting['GENOME_CACHE']):
		achr = x.split('-')[0]
		astrand = astrandmap[x.split('-')[1].split('.')[0]]
		if astrand == '-':
			sequences[genome][achr, astrand] = numpy.load(asetting['GENOME_CACHE']+x)[::-1]
		else:
			sequences[genome][achr, astrand] = numpy.load(asetting['GENOME_CACHE']+x)

# this is idiotic
dataset_to_genome = { y : x for (x,y) in genome_of_dataset.iteritems() }

homologous_gene = {}
for d in nontrivial_datasets_ordered_by_size:
	for x in numpy.where(all_SPREADS[d].NICE)[0]:
		anuorf = all_results[d].tbl[x]
		# genes marked by .N
		homolog = anuorf[1].split('.')[0]
		if homolog not in homologous_gene:
			homologous_gene[homolog] = {}
		homologous_gene[homolog].setdefault(d,[]).append(x)

# nontrivial subset
homologous_gene = { x : y for (x,y) in homologous_gene.iteritems() if len(y) > 1 }

multis = []
for a, bs in homologous_gene.iteritems():
	for b, xs in bs.iteritems():
		acc = set([])
		for x in xs:
			acc.add(all_results[b].tbl[x][1])
		if len(acc)>1:
			multis.append(a)

print 'WARNING!! The following genes have multiple unresolved homologs!!'
print multis

homologous_gene = { x : y for (x,y) in homologous_gene.iteritems() if x not in multis }


singletons = { x : y for (x,y) in homologous_gene.iteritems() if numpy.all([len(z)==1 for z in y.values()]) }


# singleton_homologs = { x : ii for (ii, x) in enumerate(singletons.keys()) }
# dataset_index = { x : ii for (ii, x) in enumerate(nontrivial_datasets_ordered_by_size) }
singleton_comparison = []
for x,ys in singletons.iteritems():
	b = []
	s = []
	uus = []
	ds = []
	for d, vs in ys.iteritems():
		# start is X nucleotides upstream of TIS
		dist_to_tis = []
		uorf_sequences = []
		for v in vs:
			anuorf = all_results[d].tbl[v]
			chrom = anuorf[0]
			strand = anuorf[-1]
			tis = per_chromosome_features_masked_out[d][chrom, strand][anuorf[1]][0]
			if strand == '+':
				dist_to_tis.append(tis - anuorf[2])
				s.append(common_formats.nicely_letters(sequences[genome_of_dataset[d]][chrom, strand][anuorf[2]:anuorf[3]]))
			else:
				dist_to_tis.append(anuorf[3] - tis)
				s.append(common_formats.nicely_letters(sequences[genome_of_dataset[d]][chrom, strand][anuorf[2]:anuorf[3]])[::-1])
			uus.append(anuorf)
		b.extend(dist_to_tis)
		ds.append(genome_of_dataset[d])
	singleton_comparison.append((x, b,s, uus, ds))

singleton_comparison = sorted(singleton_comparison, key=lambda x:numpy.std(x[1]))


# from Chiaromonte et al. (2002) SCORING PAIRWISE GENOMIC SEQUENCE ALIGNMENTS. Pacific Symposium on Biocomputing 7:115-126
blastz_subst_matrix = [[91, -114, -31, -123], [-114, 100, -125, -31], [-31, -125,100,-114], [-123, -31,-114,91]]
blastz_subst_dict = { (a,b) : c for (a,b,c) in [item for sublist in [[(l,v,k) for (k,v) in zip(vs,['A', 'C', 'G', 'T'])] for (l,vs) in zip(['A', 'C', 'G', 'T'], blastz_subst_matrix)] for item in sublist] }
blastz_gap_open = -400
blastz_gap_extend = -30

def blastz_sum_of_pairs(q):
	ingap = False
	score = 0
	for i in zip(*q):
		if i in blastz_subst_dict:
			ingap = False
			score += blastz_subst_dict[i]
		else:
			if not ingap:
				score += blastz_gap_open
				ingap = True
			else:
				score += blastz_gap_extend
	return float(score)

def blastz_sum_of_pairs_flat(q):
	ingap = False
	score = []
	wasgap = []
	for i in zip(*q):
		if i in blastz_subst_dict:
			ingap = False
			score.append(blastz_subst_dict[i])
		else:
			if not ingap:
				score.append(blastz_gap_open)
				ingap = True
			else:
				score.append(blastz_gap_extend)
		wasgap.append(int(ingap))
	return numpy.cumsum(score), wasgap


def null_sop_scores(gscores, ggaps, len_orig):
	pgaps = (numpy.where(numpy.array(ggaps)==0)[0]).astype('float')
	starti = 0
	endi = starti+len_orig
	null_sops= []
	while endi < pgaps.shape[0]:
		null_sops.append((gscores[pgaps[endi]-1]-gscores[pgaps[starti]])/(pgaps[endi]-pgaps[starti]))
		starti += 1
		endi += 1
	if len(null_sops)>1:
		return numpy.mean(null_sops), numpy.std(null_sops)
	else:
		return 6., 1.





from subprocess import Popen, PIPE

import xlwt
workbook = xlwt.Workbook() 
sheet = workbook.add_sheet("Singleton Cross-Species uORFs")
# write the column headers
for ii,x in enumerate(['', '', '', '', '']+['scer', '', '', '', '']+['spar', '', '', '', '']+['sbay', '', '', '', '']):
	sheet.write(0,ii,x)

for ii,x in enumerate(['gene', 'std(dTIS)', 'sop-utr', 'sop-upstream', 'sop-uORF']+(['chr', 'start', 'end', 'dTIS', 'asequence']*3)):
	sheet.write(1,ii,x)

goff = { 'S_cerevisiae' : 5, 'S_paradoxus' : 10, 'S_bayanus' : 15 }
kk = 2
for x in singleton_comparison:
	sheet.write(kk,0,x[0])
	sheet.write(kk,1,numpy.std(x[1]).round(3))
	# x = ['TTGACACACGAATTATATAAACGAAGTTATACAGAAAAAGATTAA', 'TTGACACGAACCGAAATAAAAGAAATTATAGTCTAA', 'TTGATTGAACACACAAAAAAACAGAAAAACTAA']
	# stupid = ''.join([item for sublist in zip(['>\n']*len(x[2]),x[2],['\n']*len(x[2])) for item in sublist])
	stupid = "\n".join(["\n".join(['>'+a[0],a[1]]) for a in zip(x[4], x[2])])
	aligns = Popen(['/usr1/home/JUMBO/muscle/muscle'], stdin=PIPE, stdout=PIPE).communicate(stupid)[0]
	msas = { b[0] : ''.join(b[1:]) for b in [a.split() for a in aligns.split('>')[1:]] }
	sheet.write(kk,4,numpy.mean([blastz_sum_of_pairs(q) for q in itertools.combinations(msas.values(),2)])/max([len(xxx) for xxx in x[2]]))
	for z in zip(x[1], x[3], x[4]):
		j = goff[z[2]]
		sheet.write(kk, j, z[1][0])
		sheet.write(kk, j+1, z[1][2])
		sheet.write(kk, j+2, z[1][3])
		sheet.write(kk, j+3, z[0])
		sheet.write(kk, j+4, msas[z[2]])
	TSS_sequences = []
	for i in range(len(x[4])):
		y = x[3][i]
		tss = per_chromosome_TLS_data[dataset_to_genome[x[4][i]]][y[0], y[4]][y[1]]
		tis, blah = per_chromosome_features_masked_out[dataset_to_genome[x[4][i]]][y[0], y[4]][y[1]]
		aa,bb = sorted([tss, tis])
		myseq = common_formats.nicely_letters(sequences[x[4][i]][y[0], y[4]][aa:bb])
		if y[4] == '-':
			myseq = myseq[::-1]
		TSS_sequences.append(myseq)
	stupid = "\n".join(["\n".join(['>'+a[0],a[1]]) for a in zip(x[4], TSS_sequences)])
	aligns = Popen(['/usr1/home/JUMBO/muscle/muscle'], stdin=PIPE, stdout=PIPE).communicate(stupid)[0]
	msas = { b[0] : ''.join(b[1:]) for b in [a.split() for a in aligns.split('>')[1:]] }
	max_tls_len = max([len(xxx) for xxx in TSS_sequences])
	sheet.write(kk,2,numpy.mean([blastz_sum_of_pairs(q) for q in itertools.combinations(msas.values(),2)])/max_tls_len)
	UPSTREAM_PROXY_sequences = []
	for i in range(len(x[4])):
		y = x[3][i]
		tis, blah = per_chromosome_features_masked_out[dataset_to_genome[x[4][i]]][y[0], y[4]][y[1]]
		if y[4] == '-':
			aa = tis
			bb = tis + max_tls_len
		else:
			aa = tis - max_tls_len
			bb = tis
		myseq = common_formats.nicely_letters(sequences[x[4][i]][y[0], y[4]][aa:bb])
		if y[4] == '-':
			myseq = myseq[::-1]
		UPSTREAM_PROXY_sequences.append(myseq)
	stupid = "\n".join(["\n".join(['>'+a[0],a[1]]) for a in zip(x[4], UPSTREAM_PROXY_sequences)])
	aligns = Popen(['/usr1/home/JUMBO/muscle/muscle'], stdin=PIPE, stdout=PIPE).communicate(stupid)[0]
	msas = { b[0] : ''.join(b[1:]) for b in [a.split() for a in aligns.split('>')[1:]] }
	sheet.write(kk,3,numpy.mean([blastz_sum_of_pairs(q) for q in itertools.combinations(msas.values(),2)])/max_tls_len)
	kk += 1


workbook.save('results/sensu-stricto-singleton-homology.xls')


### extend to other genes
def build_reverse_msa_lookup(msas):
	reverse_msa = {}
	for d in msas.keys():
		vvs = [0]*len(msas[d])
		jj = 1
		for i in range(1,len(msas[d])+1):
			if msas[d][-i]!='-':
				vvs[-i] = jj
				jj += 1
			else:
				vvs[-i] = None
		reverse_msa[d] = vvs
	return reverse_msa


projection_of_homologous_gene = []
# take the sequence of each uORF and probe against the other species' TLSs
for ii,(agene, species) in enumerate(homologous_gene.iteritems()):
	#
	# find the longest TLS 
	tls_positions = {}
	for d,vs in species.iteritems():
		v = vs[0]
		anuorf = all_results[d].tbl[v]
		chrom = anuorf[0]
		strand = anuorf[-1]
		tis = per_chromosome_features_masked_out[d][chrom, strand][anuorf[1]][0]
		tss = per_chromosome_TLS_data[d][chrom, strand][anuorf[1]]
		tls_positions[d] = tss, tis, chrom, strand
	#
	max_tls_len = max([abs(x[0]-x[1]) for x in tls_positions.itervalues()])
	#
	# now grab the upstream sequences and align
	upstream_sequences = {}
	for d, (tss, tis, chrom, strand) in tls_positions.iteritems():
		if strand == '+':
			upstream_sequences[d] = common_formats.nicely_letters(sequences[genome_of_dataset[d]][chrom, strand][tis-max_tls_len:tis])
		else:
			upstream_sequences[d] = common_formats.nicely_letters(sequences[genome_of_dataset[d]][chrom, strand][tis:tis+max_tls_len][::-1])
	#
	# align the upstream genomic sequences
	stupid = "\n".join(["\n".join(['>'+a[0],a[1]]) for a in upstream_sequences.iteritems()])
	aligns = Popen(['/usr1/home/JUMBO/muscle/muscle'], stdin=PIPE, stdout=PIPE).communicate(stupid)[0]
	msas = { b[0] : ''.join(b[1:]) for b in [a.split() for a in aligns.split('>')[1:]] }
	reverse_msa = build_reverse_msa_lookup(msas)
	msa_len = len(reverse_msa.values()[0])
	#
	# build a skiplist in each TLS+upstream region
	# we're going to truncate at the TIS if necessary
	tls_skiplist = {}
	for d,vs in species.iteritems():
		t = skippy.IntervalSkiplist(3)
		for v in vs:
			anuorf = all_results[d].tbl[v]
			chrom = anuorf[0]
			strand = anuorf[-1]
			tis = per_chromosome_features_masked_out[d][chrom, strand][anuorf[1]][0]
			if strand == '+':
				dist_to_tis = tis - anuorf[2]
				end_dist_to_tis = tis - min(anuorf[3], tis)
				t.insert_interval(v, end_dist_to_tis, dist_to_tis)
			else:
				dist_to_tis = anuorf[3] - tis
				end_dist_to_tis = max(tis, anuorf[2])-tis
				t.insert_interval(v, end_dist_to_tis, dist_to_tis)
		tls_skiplist[d] = t
	#
	# using the skiplists, assign the flanking types to each 
	flanking_types = {}
	for d, t in tls_skiplist.iteritems():
		flanking_types[d] = {}
		inorder = [list(x)[0] for x in t]
		# tis uorf(1) tss
		# tis uorf(2) uorf(3) tss
		# tis uorf(2) uorf(4) uorf(3) tss
		if len(inorder)==0:
			break
		elif len(inorder)==1:
			flanking_types[d][inorder[0]] = 1 
		elif len(inorder)==2:
			flanking_types[d][inorder[0]] = 2
			flanking_types[d][inorder[1]] = 3
		else:
			flanking_types[d][inorder[0]] = 2
			flanking_types[d][inorder[-1]] = 3
			for k in range(1, len(inorder)-1):
				flanking_types[d][inorder[k]] = 4
	#
	# calculate the suffient statistics of the sum-of-pair comparisons
	sequence_pair_data = {}
	upstream_sequence_pair_data = {}
	for d,dprime in itertools.combinations(species.keys(),2):
		[a,b] = sorted([d,dprime])
		sequence_pair_data[a,b] = blastz_sum_of_pairs_flat([msas[a], msas[b]])
		upstream_sequence_pair_data[a,b] = blastz_sum_of_pairs_flat([upstream_sequences[a], upstream_sequences[b]])
		# comparison_of_homologous_gene[agene][a,b] = sequence_pair_data[a,b][0][-1]/sequence_pair_data[a,b][0].shape[0]
	#
	# evaluate each species
	for d, t in tls_skiplist.iteritems():
		for ranuorf in t:
			# in this case we know that there is just one here
			anuorf = list(ranuorf)[0]
			end_, start_ = tls_skiplist[d].lookup[anuorf]
			anuorf_len = start_-end_
			# find these positions in the MSA of own species
			msa_end_ = reverse_msa[d].index(max(end_,1))
			msa_start_ = reverse_msa[d].index(start_)
			for dprime in tls_skiplist.keys():
				if dprime == d:
					continue
				# then project those positions through the MSA into each other species and ask
				# 1) what's the sequence similarity of what I hit?
				induced_sop = blastz_sum_of_pairs([msas[d][msa_start_:msa_end_], msas[dprime][msa_start_:msa_end_]])/(msa_end_-msa_start_)
				# z-score the SOP
				a,b = sequence_pair_data[tuple(sorted([d,dprime]))]
				mm, ss = null_sop_scores(a,b, msa_end_-msa_start_)
				induced_zscore = (induced_sop-mm)/ss
				# 2) did I hit a uorf? - if I did, how much do I overlap it?
				# aside: why does Python not have foldl/foldr? what a lousy language
				reverse_msa_without_gaps = [x for x in reverse_msa[dprime][msa_start_:msa_end_] if x is not None]
				induced_best_overlap = 0.0
				flank_of_induced_best_overlap = None
				len_of_induced_hit = None
				if len(reverse_msa_without_gaps)>0:
					induced_end_ = min(reverse_msa_without_gaps)
					induced_start_ = max(reverse_msa_without_gaps)
					candidate_induced_uorfs_hit = tls_skiplist[dprime].what_contains_interval(induced_end_, induced_start_)
					induced_uorfs_hit = None
					for ax, x in [(x, tls_skiplist[dprime].lookup[x]) for x in candidate_induced_uorfs_hit]:
						thisinduced = (min(induced_start_, x[1])-max(induced_end_, x[0]))/float(max(induced_start_, x[1])-min(induced_end_, x[0]))
						if thisinduced > induced_best_overlap:
							flank_of_induced_best_overlap = flanking_types[dprime][ax]
							induced_best_overlap = thisinduced
							induced_uorfs_hit = ax
							len_of_induced_hit = x[1]-x[0]
				else:
					# we're landing in a gap.
					induced_uorfs_hit = None
				# 3) now look at this from an absolute sequence perspective
				absolute_sop = blastz_sum_of_pairs([upstream_sequences[d][-start_:-end_],upstream_sequences[dprime][-start_:-end_]])/anuorf_len
				a,b = upstream_sequence_pair_data[tuple(sorted([d,dprime]))]
				mm, ss = null_sop_scores(a,b, anuorf_len)
				absolute_zscore = (absolute_sop-mm)/ss
				candidate_absolute_uorfs_hit = tls_skiplist[dprime].what_contains_interval(end_, start_)
				absolute_best_overlap = 0.0
				flank_of_absolute_best_overlap = None
				absolute_uorfs_hit = None
				len_of_absolute_hit = None
				for ax,x in [(x, tls_skiplist[dprime].lookup[x]) for x in candidate_absolute_uorfs_hit]:
					thisinduced = (min(start_, x[1])-max(end_, x[0]))/float(max(start_, x[1])-min(end_, x[0]))
					if thisinduced > absolute_best_overlap:
						flank_of_absolute_best_overlap = flanking_types[dprime][ax]
						absolute_best_overlap = thisinduced
						absolute_uorfs_hit = ax
						len_of_absolute_hit = x[1]-x[0]
				induced_bits = induced_sop, induced_uorfs_hit, induced_best_overlap, flank_of_induced_best_overlap, induced_zscore, len_of_induced_hit
				absolute_bits = absolute_sop, absolute_uorfs_hit, absolute_best_overlap, flank_of_absolute_best_overlap, absolute_zscore, len_of_absolute_hit
				my_bits = anuorf, anuorf_len, flanking_types[d][anuorf]
				projection_of_homologous_gene.append((agene, (d,dprime),  my_bits, induced_bits, absolute_bits))

# how often do we hit the same uorf in absolute and MSA?
hit_same_uorf = numpy.where([x[3][1]==x[4][1] and x[4][1] is not None for x in projection_of_homologous_gene])[0]
hit_not_matching_uorf = numpy.where([x[3][1]!=x[4][1] and x[3][1] is not None and x[4][1] is not None for x in projection_of_homologous_gene])[0]
not_hit_any_uorf = numpy.where([x[3][1] is None and x[4][1] is None for x in projection_of_homologous_gene])[0]
only_msa_hit = numpy.where([x[3][1] is not None and x[4][1] is None for x in projection_of_homologous_gene])[0]
only_abs_hit = numpy.where([x[3][1] is None and x[4][1] is not None for x in projection_of_homologous_gene])[0]

print 1/0

# get the start codon for each uORF
start_codons = {}
for d, rr in all_standardized_results.iteritems():
	start_codons[d] = []
	for anuorf in rr.tbl:
		if anuorf[-1]=='+':
			start_codons[d].append(common_formats.nicely_letters(sequences[genome_of_dataset[d]][anuorf[0], anuorf[-1]][anuorf[2]:anuorf[2]+3]))
		else:
			start_codons[d].append(common_formats.nicely_letters(sequences[genome_of_dataset[d]][anuorf[0], anuorf[-1]][anuorf[3]-3:anuorf[3]][::-1]))

start_codon_indices = {x : ii for (ii,x) in enumerate(['ATG']+list(set([]).union(*start_codons.values())-set(['ATG'])))}

# are uorfs distinguishable by their feature vectors?
for d in all_standardized_results.keys():
	AUG_uorfs = numpy.array([x=='ATG' for x in start_codons[d]])
	nonAUG_uorfs = ~AUG_uorfs
	FF = LogisticRegressionCV(solver='liblinear')
	FF.fit(all_standardized_results[d].feats[all_SPREADS[d].NICE], AUG_uorfs[all_SPREADS[d].NICE])
	print 'accuracy for AUG/nonAUG', d, numpy.mean(FF.predict(all_standardized_results[d].feats[all_SPREADS[d].NICE])==AUG_uorfs[all_SPREADS[d].NICE])



# induced_bits = induced_sop, induced_uorfs_hit, induced_best_overlap, flank_of_induced_best_overlap, induced_zscore, len_of_induced_hit
# absolute_bits = absolute_sop, absolute_uorfs_hit, absolute_best_overlap, flank_of_absolute_best_overlap, absolute_zscore, len_of_absolute_hit
# my_bits = anuorf, anuorf_len, flanking_types[d][anuorf]
# projection_of_homologous_gene.append((agene, (d,dprime),  my_bits, induced_bits, absolute_bits))


# look at the MSA-based analogs
# how often do we match start codon?
induced_start_codon_matches = numpy.zeros((len(start_codon_indices), len(start_codon_indices)))
for thunk in projection_of_homologous_gene:
	s1 = thunk[1][0]
	s2 = thunk[1][1]
	u1 = thunk[2][0]
	u2 = thunk[3][1]
	if u2 is not None:
		induced_start_codon_matches[start_codon_indices[start_codons[s1][u1]], start_codon_indices[start_codons[s2][u2]]] += 1

print 'strong pressure for ATG and moderate pressure for TTG to stay the same', numpy.diag(induced_start_codon_matches)/numpy.sum(induced_start_codon_matches,axis=1)

# how often do induced matches preserve flank types?
flank_type_preservation = numpy.zeros((4,4))
for thunk in projection_of_homologous_gene:
	u1 = thunk[2][0]
	u2 = thunk[3][1]
	if u2 is not None:
		flank_type_preservation[thunk[2][2]-1, thunk[3][3]-1] += 1

print 'flank_type_preservation', flank_type_preservation


x[0]
projection_of_homologous_gene[0][3]



hit_same_uorf = numpy.where([x[3][1]==x[4][1] and x[4][1] is not None for x in projection_of_homologous_gene])[0]

induced_zscore_dist = [projection_of_homologous_gene[q][3][4] for q in hit_same_uorf]
absolute_zscore_dist = [projection_of_homologous_gene[q][4][4] for x in hit_same_uorf]

pylab.plot(induced_zscore_dist, absolute_zscore_dist, 'k.', markersize=4)
pylab.savefig('tmp.png')
pylab.close()







# let's try and make classes of uorfs
# how well can I align them?
# part of the issue is that I need to decide when matches are indeed "close enough" to count
induced_zscore_dist = [x[3][4] for x in projection_of_homologous_gene if x[3][1] is not None]
absolute_zscore_dist = [x[4][4] for x in projection_of_homologous_gene if x[4][1] is not None]

pylab.hist(induced_zscore_dist, 100)
pylab.savefig('tmp.png')
pylab.close()

pylab.hist(absolute_zscore_dist, 100)
pylab.savefig('tmp.png')
pylab.close()






absolute_from_to_flanks = numpy.zeros((4,4))
induced_from_to_flanks = numpy.zeros((4,4))

for x in range(len(projection_of_homologous_gene)):
	agene, (d,dprime), my_bits, induced_bits, absolute_bits = projection_of_homologous_gene[x]
	if induced_bits[3] is not None:
		induced_from_to_flanks[my_bits[2]-1, induced_bits[3]-1] += 1
	if absolute_bits[3] is not None:
		absolute_from_to_flanks[my_bits[2]-1, absolute_bits[3]-1] += 1

numpy.sum(numpy.diag(induced_from_to_flanks))/numpy.sum(induced_from_to_flanks)
numpy.sum(numpy.diag(absolute_from_to_flanks))/numpy.sum(absolute_from_to_flanks)


absolute_from_to_flanks = numpy.zeros((4,4))
induced_from_to_flanks = numpy.zeros((4,4))

for x in only_msa_hit:
	agene, (d,dprime), my_bits, induced_bits, absolute_bits = projection_of_homologous_gene[x]
	if induced_bits[3] is not None:
		induced_from_to_flanks[my_bits[2]-1, induced_bits[3]-1] += 1
	if absolute_bits[3] is not None:
		absolute_from_to_flanks[my_bits[2]-1, absolute_bits[3]-1] += 1
	if (my_bits[2], induced_bits[3])==(1,4):
		print 1/0

absolute_from_to_flanks = {}
induced_from_to_flanks = {}

for x in only_abs_hit: #range(len(projection_of_homologous_gene)):
	agene, (d,dprime), my_bits, induced_bits, absolute_bits = projection_of_homologous_gene[x]
	if induced_bits[3] is not None:
		induced_from_to_flanks.setdefault((my_bits[2], induced_bits[3]),{}).setdefault(agene,[]).append(x)
	if absolute_bits[3] is not None:
		absolute_from_to_flanks.setdefault((my_bits[2], absolute_bits[3]),{}).setdefault(agene,[]).append(x)

count_absolute_from_to_flanks = numpy.zeros((4,4))
count_induced_from_to_flanks = numpy.zeros((4,4))

for (x,y) in induced_from_to_flanks.iteritems():
	count_induced_from_to_flanks[x[0]-1,x[1]-1] = len(y)

for (x,y) in absolute_from_to_flanks.iteritems():
	count_absolute_from_to_flanks[x[0]-1,x[1]-1] = len(y)


gene_buckets = {}
for ii,x in enumerate(projection_of_homologous_gene):
	gene_buckets.setdefault(x[0],[]).append((ii,x))


abs_types_of_homologs = {}
rel_types_of_homologs = {}
for g,xs in gene_buckets.iteritems():
	rel_me = set([])
	abs_me = set([])
	for y,x in xs:
		agene, (d,dprime), my_bits, induced_bits, absolute_bits = x
		if induced_bits[3] is not None:
			rel_me.add((my_bits[2], induced_bits[3]))
		if absolute_bits[3] is not None:
			abs_me.add((my_bits[2], absolute_bits[3]))
	abs_types_of_homologs.setdefault(frozenset(abs_me),[]).append(g)
	rel_types_of_homologs.setdefault(frozenset(rel_me),[]).append(g)

rel_types_of_homologs[frozenset([(1, 2), (2, 1)])]

sorted(abs_types_of_homologs.keys(), key=lambda x:len(abs_types_of_homologs[x]))

goofnik_types_of_homologs = {}
for g,xs in gene_buckets.iteritems():
	goof_me = set([])
	z = []
	for y,x in xs:
		agene, (d,dprime), my_bits, induced_bits, absolute_bits = x
		goof_me.add((my_bits[2], induced_bits[3], absolute_bits[3]))
		z.append(y)
	goofnik_types_of_homologs.setdefault(frozenset(goof_me),[]).append((g,z))

blahs = sorted(goofnik_types_of_homologs.keys(), key=lambda x:len(goofnik_types_of_homologs[x]), reverse=True)
[x for x in blahs if x[0]!=x[1]]

goofnik_types_of_homologs[frozenset([(3, 1, 1), (2, None, None), (1, 3, 3)])]

projection_of_homologous_gene[47]

print 1/0



# 60% of the time, ok
print float(hit_same_uorf.shape[0])/len(projection_of_homologous_gene)
# 0.4% of the time, ok
print float(hit_not_matching_uorf.shape[0])/len(projection_of_homologous_gene)
# 36% of the time, ok
print float(not_hit_any_uorf.shape[0])/len(projection_of_homologous_gene)
# 2% of the time, ok
print float(only_msa_hit.shape[0])/len(projection_of_homologous_gene)
# 1.5% of the time, ok
print float(only_abs_hit.shape[0])/len(projection_of_homologous_gene)

golden_genes = set([x[1] for x in golden.keys()])

set([projection_of_homologous_gene[z][0] for z in only_abs_hit]) & set(golden_genes)

# TOADD: Length of uORF I am to length of uORF I'm comparing to
# TOADD: z-score the absolute SOP as well

acc = {}
for a in [(projection_of_homologous_gene[z][3][3], projection_of_homologous_gene[z][4][3]) for z in hit_not_matching_uorf]:
	if a not in acc:
		acc[a] = 0
	acc[a] += 1






X = [[projection_of_homologous_gene[q][3][0], projection_of_homologous_gene[q][4][0]] for q in hit_same_uorf]
X = numpy.vstack(X)
pylab.plot(X[:,0], X[:,1], 'k.')
X = [[projection_of_homologous_gene[q][3][0], projection_of_homologous_gene[q][4][0]] for q in not_hit_any_uorf]
X = numpy.vstack(X)
pylab.plot(X[:,0], X[:,1], 'r.')
pylab.savefig('results/tmp.png')
pylab.close()


anexample = projection_of_homologous_gene[hit_not_matching_uorf[-1]]
for species, vs in homologous_gene[anexample[0]].iteritems():
	for v in vs:
		print species, v, all_results[species].tbl[v]




# X = numpy.vstack([[x[6],x[4]] for x in projection_of_homologous_gene])
# pylab.plot(X[:,0], X[:,1], 'k.')
# pylab.xlabel('overlap')
# pylab.ylabel('z-score SOP')
# pylab.savefig('results/induced-overlap_vs_zscore.png')
# pylab.close()

# X = numpy.vstack([[x[6],x[3]] for x in projection_of_homologous_gene])
# pylab.plot(X[:,0], X[:,1], 'k.')
# pylab.xlabel('overlap')
# pylab.ylabel('z-score SOP')
# pylab.savefig('results/induced-overlap_vs_sop.png')
# pylab.close()

# print 'percentage of uorfs hitting a uorf in another species through MSA', numpy.mean([x[6]>0 for x in projection_of_homologous_gene])

# print 'percentage of uorfs hitting a uorf in another species (absolute)', numpy.mean([x[-1]>0 for x in projection_of_homologous_gene])


# X = numpy.vstack([[x[3],x[-3]] for x in projection_of_homologous_gene])
# pylab.plot(X[:,0], X[:,1], 'k.')
# pylab.xlabel('induced SOP')
# pylab.ylabel('absolute SOP')
# pylab.savefig('results/induced-overlap_vs_absolute-overlap.png')
# pylab.close()


print 1/0

# TOADD: blockbuster/gasstation identities
# TOADD: 
