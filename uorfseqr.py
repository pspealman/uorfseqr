# -*- coding: utf-8 -*-
"""
Created on Thu Aug 16 16:39:54 2018

uorfseqr
Description:
This wrapper acts as an interface for the core uORFs scripts created by Armaghan Naik. 

@author: Pieter Spealman

11.12.2019 (Vein Suntan) Implementing improvements for MiMB publication
    _x_ pseudotriplicate, method to randomly sample (without replacement) two bam files into a third. Three bam files result.

12.3.2019 (Concentrate Feed) Implementing improvements for MiMB publication
    _x_ nteseqr, identify high likelihood NTEs and filter them from uORFseqr
    _x_ bug fix on newcompute
    _x_ updated path conventions
    _x_ added 'filter_uorfs' bundling
    
    
"""
import re
import numpy as np
import argparse
import subprocess
import os

def output_handler(output):
    if len(output.strip()) > 1:
        print(output)

#function handles help commands
def help_dialog():
    monolog=('Manual for uorfseqr\n'+
    '#=============================================================#\n'+
    'Lead programmer: Armaghan Naik anaik@cmu.edu\n'+
    'Assistant: Pieter Spealman pspsealman@cmu.edu \n'+
    'Release version: 1.3 \n'+
    'Release date: 08.18.18 \n'+
    'Description:\n\tuORF-seqr is novel machine learning approach that measures features of\n'+
    '\texpression (mRNA, Ribosome profiling data) of a set of molecularly validated\n'+
    '\tuORFs across triplicate datasets. These features are then used to predict\n'+
    '\tnovel uORFs.\n'+
    'Citation:\n\tSpealman, P. and Naik, A. et al. \n'+
    '\tConserved non-AUG uORFs revealed by a novel regression analysis of ribosome profiling data.\n'+
    '\tGenome Research 28, 214â€“222 (2018).\n'+
    '\tdoi: 10.1101/gr.221507.117\n'+
    'Copyright MIT License, 2017 Armaghan Naik, Pieter Spealman'
    '#=============================================================#\n'+
    'For demonstration use:\n\t python uorfseqr.py -demo\n'+
    'To run a install test using defaults, use:\n\t python uorfseqr.py -test\n'+
    '')
    print(monolog)
   
def demo():
    monolog = ('\tStep 1. -preprocess command processes genome.\n')
    print(monolog)
    monolog = ('\t\tUsage:\n\tpython uorfseqr.py -pre -genome_name <str> -fa <path to reference fasta file>\n'+
    '\t\tExample:\n\tpython uorfseqr.py -pre -genome_name s_cerervisiae -fa data/reference_genomes/Scer_SacCer3.fa\n')
    print(monolog)

    monolog = ('\tStep 2. -quantify command caluclates feature weights of known uORFs.\n')
    print(monolog)
    monolog = ('\t\tUsage:\n\tpython uorfseqr.py -quantify -genome_name <str> -samples <sample _name> <path to sample RPF.bam> <path to sample mRNA.bam> -o <output_dir>\n'+
    '\t\tExample:\n\tpython uorfseqr.py -quantify -genome_name s_cerervisiae -samples Scer_A ../data/bam/Scer_A_RPF_10.bam ../data/bam/Scer_A_mRNA_10.bam Scer_B ../data/bam/Scer_B_RPF_10.bam ../data/bam/Scer_B_mRNA_10.bam Scer_C ../data/bam/Scer_C_RPF_10.bam ../data/bam/Scer_C_mRNA_10.bam -o scer.demo')
    print(monolog)

    monolog = ('\tStep 3. -predict command generates predictions using the weighted features generated by the quantify command.\n')
    print(monolog)
    monolog = ('\t\tUsage:\n\tpython uorfseqr.py -predict -i <previous output_dir> -o <str>\n'+
    '\t\tExample:\n\tpython uorfseqr.py -predict -i scer.demo -o scer.demo/results\n')
    print(monolog)    
    
def test():
    monolog = ('=== Currently Testing uorfseqr.py ===')
    print(monolog)

    monolog = ('\tTesting Step 1. -preprocess command processes genome.\n')
    print(monolog)    
    bashCommand = ('python uorfseqr.py -preprocess -genome_name s_cerervisiae -fa data/reference_genomes/Scer_SacCer3.fa')
    print(bashCommand)
    output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
    
    monolog = ('\tTesting Step 2. -quantify command caluclates feature weights of known uORFs.\n')
    print(monolog)    
    bashCommand = ('python uorfseqr.py -quantify -o scer.demo -genome_name s_cerervisiae -samples Scer_A ../data/bam/Scer_A_RPF_10.bam ../data/bam/Scer_A_mRNA_10.bam Scer_B ../data/bam/Scer_B_RPF_10.bam ../data/bam/Scer_B_mRNA_10.bam Scer_C ../data/bam/Scer_C_RPF_10.bam ../data/bam/Scer_C_mRNA_10.bam -o scer.demo')
    print(bashCommand)
    output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
    
    monolog = ('\tTesting Step 3. -predict command generates predictions using the weighted features generated by the quantify command.\n')
    print(monolog)    
    bashCommand = ('python uorfseqr.py -predict -i scer.demo -o scer.demo/results')
    print(bashCommand)
    output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))

### Argparser definitions        
parser = argparse.ArgumentParser()
#handle help_dialog
parser.add_argument('-man',"--manual", action='store_true')
parser.add_argument('-demo',"--demo",action='store_true')
parser.add_argument('-test',"--test",action='store_true')

#handle preprocess task
parser.add_argument('-preprocess',"--preprocess_genome", action='store_true')
parser.add_argument('-genome_name',"--genome_name")
parser.add_argument('-fa',"--fa_file")

#handle quantify task
parser.add_argument('-quantify',"--quantify_features", action='store_true')
parser.add_argument('-gff',"--gff_file")
parser.add_argument('-filter',"--filter_uorfs", nargs='+')
parser.add_argument('-known',"--known_uorfs")
parser.add_argument('-samples', '--sample_list', nargs='+')
parser.add_argument('-max_rpf','--max_rpf_size')
parser.add_argument('-min_rpf','--min_rpf_size')
parser.add_argument('-non_start', '--non_start_ncc', nargs='+')
parser.add_argument('-morf_mask', '--morf_mask')
parser.add_argument('-min_utr', '--too_small_utr')
parser.add_argument('-min_rpf_ct', '--min_rpf_ct')
parser.add_argument('-kozak_seq','--kozak_seq')
parser.add_argument('-kozak_cutoff', '--kozak_cutoff')
parser.add_argument('-in_phase_cutoff','--in_phase_cutoff')
parser.add_argument('-o',"--output_dir")

#handle predict task
parser.add_argument('-predict',"--predict_uorfs", action='store_true')
parser.add_argument('-i',"--input_dir")

#handle tools
##pseudo_triplicate
parser.add_argument('-pseudo',"--pseudo_triplicate", action='store_true')
parser.add_argument('-rep1',"--replicate_1")
parser.add_argument('-rep2',"--replicate_2")

##nteseqr
#python uorfseqr.py -nte -fa data/reference_genomes/Scer_SacCer3.fa -samples r2 r2_RPF.bam r2_RNA.bam r3 r3_RPF.bam r3_RNA.bam -o nte_demo

parser.add_argument('-nte',"--nte_seqr", action='store_true')
parser.add_argument('-gt',"--gene_tag")
parser.add_argument('-tl',"--transcript_leader_tag")
parser.add_argument('-3p',"--three_prime_UTR_tag")
parser.add_argument('-min_tl',"--minimum_length_transcript_leader")
parser.add_argument('-min_3p',"--minimum_length_three_prime_UTR")
parser.add_argument('-mask_tl',"--mask_length_transcript_leader")
parser.add_argument('-mask_3p',"--mask_length_three_prime_UTR")
parser.add_argument('-defualt',"--default_search_region_length")

#
args = parser.parse_args()
###

original_directory = os.getcwd()

if args.manual:
    help_dialog()
    
if args.demo:
    demo()
    
if args.test:
    test()

###        
def parse_cigar(cigar, sequence):
    """This function calculates the offset for the read based on the match
    """
    # TODO - maybe improve to handle '28M1I4M', 'TCAGGGAAATATTGATTTACCCAAAAAAAGACG'
    #
    if cigar.count('M') == 1:
        #print(cigar, sequence)
        left_cut = 0
        right_cut = 0
        
        left_list = re.split('M|S|D|I|H|N', cigar.split('M')[0])[0:-1]
        M = re.split('M|S|D|I|H|N', cigar.split('M')[0])[-1]
        right_list = re.split('M|S|D|I|H|N', cigar.split('M')[1])
        
        for each in left_list:
            if each: 
                left_cut += int(each)
                
        for each in right_list:
            if each: 
                right_cut -= int(each)
        
        n_cigar = ('{}M').format(M)

        if right_cut:
            n_sequence = sequence[left_cut:right_cut]
        else:
            n_sequence = sequence[left_cut:]
                        
        #print (left_cut, right_cut,  n_cigar, n_sequence)
        return(True, n_cigar, n_sequence)
            
    else:
        return(False, '', '')

def unpackbits(x, num_bits=12):
    xshape = list(x.shape)
    x = x.reshape([-1,1])
    to_and = 2**np.arange(num_bits).reshape([1,num_bits])
    upb = (x & to_and).astype(bool).astype(int).reshape(xshape + [num_bits])

    #0  (rp)    read_paired
    #1  (rmp)    read_mapped_in_proper_pair
    #2  (ru)    read_unmapped
    #3  (mu)    mate_unmapped
    #4  (rrs)    read_reverse_strand
    #5  (mrs)    mate_reverse_strand
    #6  (fip)    first_in_pair
    #7  (sip)    second_in_pair
    #8  (npa)    not_primary_alignment
    #9  (rfp)    read_fails_platform
    #10 (pcr)    read_is_PCR_or_optical_duplicate
    #11 (sa)    supplementary_alignment
    
    """ DISCORDANT definition (from samblaster)
        Both side of the read pair are mapped (neither FLAG 0x4 or 0x8 is set).
        The properly paired FLAG (0x2) is not set.
        Note: We implemented an additional criteria to distinguish between strand re-orientations and distance issues
        Strand Discordant reads must be both on the same strand.
    """
        
    """ SPLIT READS
        Identify reads that have between two and --maxSplitCount [2] primary and supplemental alignments.
        Sort these alignments by their strand-normalized position along the read.
        Two alignments are output as splitters if they are adjacent on the read, and meet these criteria:
            each covers at least --minNonOverlap [20] base pairs of the read that the other does not.
            the two alignments map to different reference sequences and/or strands. 
            the two alignments map to the same sequence and strand, and represent a SV that is at least --minIndelSize [50] in length, 
            and have at most --maxUnmappedBases [50] of un-aligned base pairs between them.
        Split read alignments that are part of a duplicate read will be output unless the -e option is used.
    """
    
    return(upb)     

def make_dir(dir_name):
    bash_command = ('mkdir -p {}').format(dir_name)
    output = subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True)
    print(output)
    
def dir_handler()  :
    original_wd = os.getcwd()
        
    if args.preprocess_genome:
        cache = args.genome_name
        if cache[-1] != '/':
            #cache = cache + '/'
            outdir_name = str(original_wd + '/data/processed_genomes/' + cache)
        else:
            outdir_name = cache
        
        make_dir(outdir_name)
    
        return(outdir_name, original_wd)
        
    if args.quantify_features:
        cache = args.output_dir
        if cache[-1] != '/':
            outdir_name = str(original_wd + '/' + cache)
        else:
            outdir_name = cache
        
        make_dir(outdir_name)
            
        return(outdir_name, original_wd)
      
def json_list_formater(original_list):
    #json format requires double quotes in lists.
    ct = 0
    output_str = ""
    for each in original_list:
        if ct < len(original_list)-1:
            each = str('"'+each+'",')
        else:
            each = str('"'+each+'"')
        output_str+=each
        ct+=1
    return(output_str)

###

### Handle inputs and defaults: 
'''
users can set their own gff file to define 5'UTRs (aka. transcript leaders), 3'UTRs,
 Transcription start sites, poly-a sites, and main orf coordinates.
 Otherwise the standard gff for S.cerevisiae (from Spealman and Naik, Genome Research, 2017) is loaded.
'''
if args.gff_file:
    gff_file = args.gff_file
else:
    gff_file = ('{}/data/reference_genomes/saccharomyces_cerevisiae.gff').format(original_directory)

'''
 users can set their own genes or regions to be filtered by loading a bed file
 otherwise the scer_baduorf file can be loaded
 NOTE: if it is desired that no genes or regions be filtered a blank file should 
  be loaded instead

 12.19.2019 - now identifies if mutltiple filter files are being used. If so it will create 
 a new file containing all unique entries.
 
'''
if args.filter_uorfs:
    if len(args.filter_uorfs) == 1:
        filter_uorfs = args.filter_uorfs[0]
    else:
        temp_filter_uorf_file_name = ('{}_temp_filter_uorf.bed').format(args.output_dir)
        temp_filter_uorf_file = open(temp_filter_uorf_file_name, 'w')
        
        #preventing exact duplicates
        line_set = set()
        
        for each_file in filter_uorfs:
            infile = open(each_file)
            
            for line in infile:
                line_set.add(line)
                
            infile.close()
            
        for each_line in line_set:
            temp_filter_uorf_file.write(each_line)
        temp_filter_uorf_file.close()
        
    filter_uorfs = temp_filter_uorf_file_name
        
else:
    filter_uorfs = ('{}/data/labelled_uorfs/scer_baduorf.bed').format(original_directory)
    
''' users can set their own list of labelled positive uORFs by loading a bed file containing
 the uORF from start codon to stop codon and strand.
 Otherwise the STANDARD-golden bed file is loaded. This file contains 17 molecularly 
    validated uORFs identified in S.cerevisiae before 2017.
 A longer list of containing the 432 predicted S.cerevisae uORFs from Spealman and Naik (Genome Research, 2017)
   is also available as 'data/labelled_uorfs/Spealman_Naik_2017.bed'
'''         
if args.known_uorfs:
    known_uorfs = args.known_uorfs
else:
    known_uorfs = ('{}/data/labelled_uorfs/STANDARD-golden.bed').format(original_directory)

#handles maximum RPF size as input, else defaults to 33bp
if args.max_rpf_size:
    maxseqlen = int(args.max_rpf_size)
else:
    maxseqlen = 33

#handles minimum RPF size as input, else defaults to 27bp
if args.min_rpf_size:
    minseqlen = int(args.min_rpf_size)
else:
    minseqlen = 27

#handles triplet nucleotids that will not be considered as start codons:
if args.non_start_ncc:
    ignore_list = args.non_start_ncc
    
else:
    ignore_list = ["AGG","AAG"]
    
#json format requires double quotes in lists.
ignore_str = json_list_formater(ignore_list)

#The user can define the canonical Kozak sequence that is used for evaluating the
# Kozak context around any potential start codon.
# The default setting is the Kozak consensus sequence defined for S. cerevisiae
if args.kozak_seq:
    kozak_seq = args.kozak_seq
else:
    kozak_seq = '"AAAAAAATGT"'
    
#The user can set the Kozak distance cutoff for evaluation.
# Otherwise it is set to 0.0001
if args.kozak_cutoff:
    kozak_cutoff = float(args.skozak_cutoff)
else:
    kozak_cutoff = 0.0001

#The user can set the cutoff for in_phase (FFT) ribosome densities.
# Otherwise this value is set to 1.0
if args.in_phase_cutoff:
    in_phase_cutoff = float(args.in_phase_cutoff)
else:
    in_phase_cutoff = 1.0
    
#allows for the user to set the main orf mask, the region upstream of the main orf
# TIS that is masked out so as to not count ribosomes actually initiating at the 
# main ORF TIS. 
if args.morf_mask:
    ribosome_overhang = int(args.morf_mask)
else:
    ribosome_overhang = 15

#allows for the user to set the minimum length of UTR (transcript leader), 
# otherwise set to 15bp
if args.too_small_utr:
    too_small_utr =  int(args.too_small_utr)
else:
    too_small_utr = 15

#allows for the user to set the minimum number of rpf reads required.
# otherwise set to 3 read minimum     
if args.min_rpf_ct:
    min_rpf = int(args.min_rpf_ct)
else:
    min_rpf = 3

##
##
##

# Task Preprocess - handles the initial setup of genome, seqeunce scans, etc.
if args.preprocess_genome:
    outdir_name, original_wd = dir_handler()      
    
    os.chdir(outdir_name)
    
    outfile = open('preprocess.cmd','w')
    
    is_fa_file = args.fa_file
    if is_fa_file[0]!='/':
        is_fa_file = str(original_wd + '/'+ args.fa_file)
    
    outline = ('genome {}\n'+
    'cachedir CACHE').format(is_fa_file)
    outfile.write(outline)
    
    outfile.close()
    
    bash_command = ('python {}/analysis/preprocess_genome.py preprocess.cmd').format(original_directory)
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    
    os.chdir(original_wd)

# Task Quantify - handles feature scoring and identification of potential uORFs 
if args.quantify_features:    
    def parse_sample_list(analysis_file, runmode):
        
        if args.sample_list:
            if len(args.sample_list)%3!= 0:
                print('Each sample requires a Name, RPF bam file, and RNA bam file.')
                
            else:
                if runmode == 'names':
                    name_list = []
                    
                    for i in range(int(len(args.sample_list)/3)):
                        name_list.append(args.sample_list[i*3])
                    
                    return(name_list)
                    
                if runmode == 'deets':
                    ct = 0
                    for i in range(int(len(args.sample_list)/3)):
                        name = args.sample_list[i*3]
                        RPF_name = args.sample_list[(i*3)+1]
                        RNA_name = args.sample_list[(i*3)+2]
                        head_bracket = '\t\t{\n'
                        sample_midline = ('\t\t\t"name":"{}",\n\t\t\t"RPF":"{}",\n\t\t\t"mRNA":"{}"\n').format(name,RPF_name,RNA_name)
                        if ct < (len(args.sample_list)/3)-1:
                            tail_bracket = '\t\t},\n'
                        else:
                            tail_bracket = '\t\t}\n'
                        sample_outline = str(head_bracket + sample_midline + tail_bracket)
                        
                        analysis_file.write(sample_outline)
                        ct+=1
                    return('Samples loaded in:\n\t\t analysis.json')
                    
        else:
            print('A sample list must be provided.\nEach sample requires a Name, RPF bam file, and RNA bam file.\n')
            
    def write_analysis_json(outdir_name):        
        analysis_file = open('analysis.json','w')

        sample_name_list = parse_sample_list(analysis_file, 'names')
        fixed_sample_name_str = json_list_formater(sample_name_list)
        
        prefix_str = ('\t"PREFIX":"{}/",\n').format(outdir_name)
        genome_str = ('\t"GENOME":"{}",\n').format(args.genome_name)
        
        genome_cache_str = ('\t"GENOME_CACHE":"../data/processed_genomes/{}/CACHE/",\n').format(args.genome_name)
        
        chromo_str = ('\t"chromosomes":["chrI","chrII","chrIII","chrIV","chrV","chrVI","chrVII","chrVIII","chrIX","chrX","chrXI","chrXII","chrXIII","chrXIV","chrXV","chrXVI"],\n')
        gff_str = ('\t"GFF_FILE":"{}",\n').format(gff_file)
        bad_uorfs_str = ('\t"BADuORFs":"{}",\n').format(filter_uorfs)
        in_phase_str=('\t"in_phase_tolerance":1.0,\n').format(in_phase_cutoff)
        weak_str=('\t"weak_kozak_cutoff":{},\n').format(kozak_cutoff)
        kozak_str=('\t"example_kozak_sequences":[{}],\n').format(kozak_seq)
        labeled_str=('\t"labeled_data":["{}"],\n').format(known_uorfs)
        rpf_max_str=('\t"rpf_maxseqlen":{},\n').format(maxseqlen)
        rpf_min_str=('\t"rpf_minseqlen":{},\n').format(minseqlen)
        processed_dir_str=('\t"processed_dir":"processed/",\n')
        ignore_start_codons_str=('\t"ignore_start_codons":[{}],\n').format(ignore_str)
        
        sample_group = outdir_name.rsplit('/',1)[1]
        sample_head = ('\t"SAMPLE_GROUPS":\n\t[\n\t\t{\n')
        sample_mid = ('\t\t\t"groupname": "{}",\n\t\t\t"wsamples": [{}]\n').format(sample_group, fixed_sample_name_str)
        sample_tail = ('\t\t}\n\t],\n')
        sample_str = ('\t"SAMPLES":\n\t[\n')

        outline = str('{\n'+prefix_str + genome_str + genome_cache_str + chromo_str + gff_str + bad_uorfs_str + 
        in_phase_str + weak_str + kozak_str + labeled_str + rpf_max_str + rpf_min_str + processed_dir_str + 
        ignore_start_codons_str + sample_head + sample_mid + sample_tail + sample_str)        
        analysis_file.write(outline)

        parse_samples = parse_sample_list(analysis_file, 'deets')
        
        sample_close_str = ('\t],\n')
        poisson_str = ('\t"poisson_random":false,\n')
        permutation_str = ('\t"permutation_seeds":[3,5,7,11],\n')
        overhang_str = ('\t"OVERHANG_BY_RIBOSOME":{},\n').format(ribosome_overhang)
        too_small_str = ('\t"TOO_SMALL_UTR":{},\n').format(too_small_utr)
        min_rpf_str = ('\t"MIN_RPF_CONSIDERED":{},\n').format(min_rpf)
        permute_5utr_str = ('\t"PERMUTE_5pUTR":false,\n')
        permute_frame_str = ('\t"PERMUTE_FRAME":null,\n')
        force_labelled_str = ('\t"FORCE_ONLY_LABELED":false,\n')
        max_orf_skip_str = ('\t"MAX_ORF_SKIPLIST":4\n')
        
        outline = str(sample_close_str + poisson_str + permutation_str + overhang_str 
        + too_small_str + min_rpf_str + permute_5utr_str + permute_frame_str 
        + force_labelled_str + max_orf_skip_str+'}\n')        
        analysis_file.write(outline)
        
        print(parse_samples)
        
        analysis_file.close()    
        
    outdir_name, original_wd = dir_handler()
    
    outline = ('Preparing run in {}...').format(outdir_name)
    print(outline)
    
    bash_command = ('mkdir -p {}').format(outdir_name)
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
        
    bash_command = ('cp {}/analysis/runprocess.py {}/.').format(original_directory, outdir_name)
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))

    bash_command = ('cp {}/analysis/runquantify.py {}/.').format(original_directory, outdir_name)
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))

    bash_command = ('cp {}/analysis/runattempt.py {}/.').format(original_directory, outdir_name)
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
            
    os.chdir(outdir_name)
    
    bash_command = ('mkdir -p bedgraph')
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    
    bash_command = ('mkdir -p curvegraph')
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    
    bash_command = ('mkdir -p processed')
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))

    write_analysis_json(outdir_name)
    
    outline = ('\n\n\nRunning initial processing on {}...').format(outdir_name)
    print(outline)
    #sudo
    bash_command = ('python runprocess.py')
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    
    outline = ('\n\n\nRunning feature quantification step on known uORFs on {}...').format(outdir_name)
    print(outline)
    #sudo
    bash_command = ('python runquantify.py')
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    
    outline = ('\n\n\nIdentifying potential uORFs in {}...').format(outdir_name)
    print(outline)
    #sudo
    bash_command = ('python runattempt.py')
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    
    outline = ('Computing features on {}...').format(outdir_name)
    print(outline)
    #sudo
    bash_command = ('python ../analysis/newcompute_features.py')
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    
    print('Completed -quantify step')
    
# Task Predict - evaluates the potential uORFs by observed score and null model scores
if args.predict_uorfs:
#sudo
    bash_command = ('python analysis/homology.py -i {}').format(args.input_dir)
        
    if args.known_uorfs:
        add_str = (' -known {}').format(args.known_uorfs)
        bash_command += add_str
        
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    #
    bash_command = ('cp results/{}-significant-resolved-qvals.bed {}/{}-candidate_uORFs.bed').format(args.input_dir, args.input_dir, args.input_dir)
    output_handler(subprocess.check_output([bash_command],stderr=subprocess.STDOUT,shell=True))
    
    print('Completed -predict step')
	
#Tools
''' nteseqr
    Given a gff and fasta identity the upstream in frame stops, 
    then downstream inframe starts, 
    then ask if reads from a sam map to those starts. 
'''
    
if args.nte_seqr:    
    monolog = ('\tRunning nteseqr, please be patient... \n')
    print(monolog)  
    if len(args.sample_list) < 6:
        print('\tnteseqr currently requires at least 2 replicates.\n'+
              '\tPlease enter each in the following format:\n'+
              '\t\t <sample_1_name> <path_to_sample_1_RPF_bam> <path_to_sample_2_RPF_bam>'+
              '<sample_1_name> <path_to_sample_1_RPF_bam> <path_to_sample_2_RPF_bam> \n')
        quit()
        
    sample_name_in = ''
    if (len(args.sample_list) % 3) == 0:
        for index in range((len(args.sample_list)//3)):
            sample_name = args.sample_list[index*3]
            print(index, sample_name)
            sample_name_str = ('{output_dir}_{sample_name}').format(output_dir=args.output_dir, sample_name=sample_name)
            sample_name_in += (sample_name_str + ' ')
            
            monolog = ('\tRunning nteseqr, please be patient... \n')
            print(monolog)
            
            rpf_name = args.sample_list[index*3+1]
            rna_name = args.sample_list[index*3+2]
            
            bashCommand = ('python {original_directory}/analysis/nteseqr.py '+
                           '-load -gff {gff_file} -fa {fa_file} -samples '+
                           '{sample_name} {rpf_name} {rna_name} -o {sample_name_str}').format(original_directory=original_directory, gff_file=gff_file, fa_file=args.fa_file, sample_name=sample_name, rpf_name=rpf_name, rna_name=rna_name, sample_name_str=sample_name_str)
            print(bashCommand)
            output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
            
        bashCommand = ('python {original_directory}/analysis/nteseqr.py -eval '+
                       '-samples {sample_name_in} -o {output_dir}').format(original_directory=original_directory, sample_name_in=sample_name_in, output_dir=args.output_dir)
        print(bashCommand)
        output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
        
        print('nteseqr completed. Candidate NTEs saved in {output_dir}.bed').format(output_dir=args.output_dir)

## Pseudo_triplicate - given one or two bam files, make three - each containing only a third of the total data.     
def load_replicate(rep_name, replicate_union_dict):
    ct = len(replicate_union_dict)
    
    monolog = ('\tLoading bam file {}.\n').format(str(rep_name))
    print(monolog)    
    bashCommand = ('samtools view -h -o temp.sam {}').format(rep_name)
    #print(bashCommand)
    output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
    
    header_list = []    
    sam_file = open('temp.sam')

    for line in sam_file:
        if line[0] == '@':
            header_list.append(line)
        else:
            uid = line.split('\t')[0]+'~'+str(ct)
            replicate_union_dict[uid] = line
            ct += 1
            
    sam_file.close()
    
    print('\tRemoving intermediate sam file.\n')
    bashCommand = ('rm temp.sam')
    output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
    
    return(header_list, replicate_union_dict)
    
def make_pseudoreplicates(replicate_union_dict, output_filename, header_list):
    
    monolog = ('\tSubsampling replicates.\n')
    print(monolog)    
    
    piece_meal = {0:{},1:{},2:{}}
    
    ct = 0
    for uid, line in replicate_union_dict.items():
        piece_meal[ct%3][uid]=line
        ct += 1
    
    monolog = ('\t{} total reads seperated into files with {}, {}, and {} reads.\n').format(ct, len(piece_meal[0]), len(piece_meal[1]), len(piece_meal[2]))
    print(monolog)    

    print('\tWriting out sam files.\n')  
    
    for each in range(3):
        sample_name = ('{}_pseudorep_{}').format(output_filename, each)
        
        outfile_name = (sample_name + '.sam')
        outfile = open(outfile_name, 'w')
        
        for line in header_list:
            outfile.write(line)
        
        outline = ('@CO\t This file is a psuedoreplicate generated using uorfseqr.\n')
        outfile.write(outline)
        
        for uid, line in piece_meal[each].items():
            outfile.write(line)
            
        outfile.close()
        
        print('\tConverting to bam file\n') 
        bashCommand = ('samtools view -Sb {sample_name}.sam > {sample_name}_unsorted.bam').format(sample_name=sample_name)
        output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
        
        bashCommand = ('samtools sort -o {sample_name}.bam {sample_name}_unsorted.bam').format(sample_name=sample_name)
        output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
        
        bashCommand = ('samtools index {sample_name}.bam').format(sample_name=sample_name)
        output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
        
        print('\tRemoving intermediate files.\n')
        bashCommand = ('rm {sample_name}.sam').format(sample_name=sample_name)
        output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
        
        bashCommand = ('rm {sample_name}_unsorted.bam').format(sample_name=sample_name)
        output_handler(subprocess.check_output([bashCommand],stderr=subprocess.STDOUT,shell=True))
        
if args.pseudo_triplicate:
    replicate_union_dict={}
    
    if args.replicate_1:
        header_list, replicate_union_dict = load_replicate(args.replicate_1, replicate_union_dict)
    
    if args.replicate_2:
        header_list, replicate_union_dict = load_replicate(args.replicate_2, replicate_union_dict)
    
    if replicate_union_dict:
        make_pseudoreplicates(replicate_union_dict, args.output_dir, header_list)
        
    else:
        print('File error. Please specify at least one bam file.')
        
